{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom __future__ import print_function\n\nfrom tensorboard import summary\nfrom six.moves import range\nfrom PIL import Image\n\nfrom torch.autograd import Variable\n\nfrom copy import deepcopy\nfrom torch.nn import init\n\n#!pip install torchfile\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# ------------------------------------------------  dataset.py --------------------------------------------------\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport PIL\nimport os\nimport os.path\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\n\nclass TextDataset(data.Dataset):\n    def __init__(self, data_dir, split='train', embedding_type='cnn-rnn',\n                 imsize=256, transform=None, target_transform=None):\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.imsize = imsize\n        self.data = []\n        self.data_dir = data_dir\n        if data_dir.find('birds') != -1:\n            self.bbox = self.load_bbox()\n        else:\n            print('checking...')\n            self.bbox = None\n        split_dir = os.path.join(data_dir, split)\n\n        self.filenames = self.load_filenames(split_dir)\n        self.embeddings = self.load_embedding(split_dir, embedding_type)\n        #self.class_id = self.load_class_id(split_dir, len(self.filenames))\n        # self.captions = self.load_all_captions()\n\n    def get_img(self, img_path, bbox):\n        img = Image.open(img_path).convert('RGB')\n        width, height = img.size\n        if bbox is not None:\n            R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n            y1 = np.maximum(0, center_y - R)\n            y2 = np.minimum(height, center_y + R)\n            x1 = np.maximum(0, center_x - R)\n            x2 = np.minimum(width, center_x + R)\n            img = img.crop([x1, y1, x2, y2])\n        load_size = int(self.imsize * 76 / 64)\n        img = img.resize((load_size, load_size), PIL.Image.BILINEAR)\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def load_bbox(self):\n        data_dir = self.data_dir\n        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n        df_bounding_boxes = pd.read_csv(bbox_path,\n                                        delim_whitespace=True,\n                                        header=None).astype(int)\n        #\n        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n        df_filenames = \\\n            pd.read_csv(filepath, delim_whitespace=True, header=None)\n        filenames = df_filenames[1].tolist()\n        print('Total filenames: ', len(filenames), filenames[0])\n        #\n        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n        numImgs = len(filenames)\n        for i in xrange(0, numImgs):\n            # bbox = [x-left, y-top, width, height]\n            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n\n            key = filenames[i][:-4]\n            filename_bbox[key] = bbox\n        #\n        return filename_bbox\n\n    def load_all_captions(self):\n        caption_dict = {}\n        for key in self.filenames:\n            caption_name = '%s/text/%s.txt' % (self.data_dir, key)\n            captions = self.load_captions(caption_name)\n            caption_dict[key] = captions\n        return caption_dict\n\n    def load_captions(self, caption_name):\n        cap_path = caption_name\n        with open(cap_path, \"r\") as f:\n            captions = f.read().decode('utf8').split('\\n')\n        captions = [cap.replace(\"\\ufffd\\ufffd\", \" \")\n                    for cap in captions if len(cap) > 0]\n        return captions\n\n    def load_embedding(self, data_dir, embedding_type):\n        if embedding_type == 'cnn-rnn':\n            print('embedding....')\n            embedding_filename = os.path.join(data_dir, 'char-CNN-RNN-embeddings.pickle')\n            #print(embedding_filename)\n            #embedding_filename = '/char-CNN-RNN-embeddings.pickle'\n        elif embedding_type == 'cnn-gru':\n            embedding_filename = '/char-CNN-GRU-embeddings.pickle'\n        elif embedding_type == 'skip-thought':\n            embedding_filename = '/skip-thought-embeddings.pickle'\n\n        with open(embedding_filename, 'rb') as f:\n            #embeddings = pickle.load(f)\n            embeddings = pickle._Unpickler(f)\n            embeddings.encoding = 'latin1'\n            embeddings = embeddings.load()[1:500]\n            #print(embeddings)\n            embeddings = np.array(embeddings)\n            print('embeddings: ', embeddings.shape)\n            # embedding_shape = [embeddings.shape[-1]]\n        '''with open('mnist.pkl', 'rb') as f:\n            u = pickle._Unpickler(f)\n            u.encoding = 'latin1'\n            p = u.load()'''\n            \n            \n        return embeddings\n\n    '''def load_class_id(self, data_dir, total_num):\n        if os.path.isfile(data_dir + '/class_info.pickle'):\n            with open(data_dir + '/class_info.pickle', 'rb') as f:\n                class_id = pickle.load(f)\n        else:\n            class_id = np.arange(total_num)\n        return class_id'''\n    \n\n    def load_filenames(self, data_dir):\n        #../input/coco-data/coco/coco/train/filenames.pickle\n        filepath = os.path.join(data_dir, 'filenames.pickle')\n        with open(filepath, 'rb') as f:\n            filenames = pickle.load(f)[1:500]\n        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n        print(len(filenames))\n        #print(args.IMG_DIR)\n        #l=os.listdir('../input/coco-train-val2017/train2014/train2014')\n        #im_file=[x.split('.')[0] for x in l]\n        #im_file = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"args.IMG_DIR\") if os.path.isfile(os.path.join('args.IMG_DIR', x))]\n        #print(len(im_file))\n        #str = 'COCO_train2014_'\n        #im_file = [str + x for x in im_file]\n        #print(im_file[0]) \n        print(filenames[0]) #COCO_train2014_\n        #te = list(set(im_file) & set(filenames)) \n        #print('te: ', len(te))\n        #list_ = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"path/to/Data\") if os.path.isfile(os.path.join('path/to/Data', x))]\n        #print(ssd)\n        return filenames\n\n    \n    '''def load_filenames(self, data_dir):\n        #../input/coco-data/coco/coco/train/filenames.pickle\n        filepath = os.path.join(data_dir, 'filenames.pickle')\n        with open(filepath, 'rb') as f:\n            filenames = pickle.load(f)\n        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n        return filenames'''\n\n    def __getitem__(self, index):\n        key = self.filenames[index]\n        # cls_id = self.class_id[index]\n        #\n        if self.bbox is not None:\n            bbox = self.bbox[key]\n            data_dir = '%s/CUB_200_2011' % self.data_dir\n        else:\n            bbox = None\n            data_dir = self.data_dir\n\n        # captions = self.captions[key]\n        embeddings = self.embeddings[index, :, :]\n        #print(args.IMG_DIR)\n        #print(key)\n        #y = key[-12:]\n        img_name = '%s/%s.jpg' % (args.IMG_DIR, key)\n        #img_name = '%s/images/%s.jpg' % (data_dir, key)\n        img = self.get_img(img_name, bbox)\n\n        embedding_ix = random.randint(0, embeddings.shape[0]-1)\n        embedding = embeddings[embedding_ix, :]\n        if self.target_transform is not None:\n            embedding = self.target_transform(embedding)\n        return img, embedding\n\n    def __len__(self):\n        return len(self.filenames)\n\n# ------------------------------------------------  utils.py --------------------------------------------------\n\nimport os\nimport errno\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.utils as vutils\n\n#############################\ndef KL_loss(mu, logvar):\n    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n    KLD = torch.mean(KLD_element).mul_(-0.5)\n    return KLD\n\n\ndef compute_discriminator_loss(netD, real_imgs, fake_imgs,\n                               real_labels, fake_labels,\n                               conditions, gpus):\n    criterion = nn.BCELoss()\n    batch_size = real_imgs.size(0)\n    cond = conditions.detach()\n    fake = fake_imgs.detach()\n    real_features = nn.parallel.data_parallel(netD, (real_imgs), gpus)\n    fake_features = nn.parallel.data_parallel(netD, (fake), gpus)\n    # real pairs\n    inputs = (real_features, cond)\n    real_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_real = criterion(real_logits, real_labels)\n    # wrong pairs\n    inputs = (real_features[:(batch_size-1)], cond[1:])\n    wrong_logits = \\\n        nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_wrong = criterion(wrong_logits, fake_labels[1:])\n    # fake pairs\n    inputs = (fake_features, cond)\n    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_fake = criterion(fake_logits, fake_labels)\n\n    if netD.get_uncond_logits is not None:\n        real_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (real_features), gpus)\n        fake_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (fake_features), gpus)\n        uncond_errD_real = criterion(real_logits, real_labels)\n        uncond_errD_fake = criterion(fake_logits, fake_labels)\n        #\n        errD = ((errD_real + uncond_errD_real) / 2. + (errD_fake + errD_wrong + uncond_errD_fake) / 3.)\n        errD_real = (errD_real + uncond_errD_real) / 2.\n        errD_fake = (errD_fake + uncond_errD_fake) / 2.\n    else:\n        errD = errD_real + (errD_fake + errD_wrong) * 0.5\n    return errD, errD_real.data, errD_wrong.data, errD_fake.data\n\n\ndef compute_generator_loss(netD, fake_imgs, real_labels, conditions, gpus):\n    criterion = nn.BCELoss()\n    cond = conditions.detach()\n    fake_features = nn.parallel.data_parallel(netD, (fake_imgs), gpus)\n    # fake pairs\n    inputs = (fake_features, cond)\n    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_fake = criterion(fake_logits, real_labels)\n    if netD.get_uncond_logits is not None:\n        fake_logits = \\\n            nn.parallel.data_parallel(netD.get_uncond_logits,\n                                      (fake_features), gpus)\n        uncond_errD_fake = criterion(fake_logits, real_labels)\n        errD_fake += uncond_errD_fake\n    return errD_fake\n\n\n#############################\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.fill_(0.0)\n\n\n#############################\ndef save_img_results(data_img, fake, epoch, image_dir):\n    num = args.VIS_COUNT\n    fake = fake[0:num]\n    # data_img is changed to [0,1]\n    if data_img is not None:\n        data_img = data_img[0:num]\n        vutils.save_image(data_img, '%s/real_samples.png' % image_dir, normalize=True)\n        # fake.data is still [-1, 1]\n        vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n    else:\n        vutils.save_image(fake.data, '%s/lr_fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n\n\ndef save_model(netG, netD, epoch, model_dir):\n    torch.save(\n        netG.state_dict(),\n        '%s/netG_epoch_%d.pth' % (model_dir, epoch))\n    torch.save(\n        netD.state_dict(),\n        '%s/netD_epoch_last.pth' % (model_dir))\n    print('Save G/D models')\n\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n# ------------------------------------------------  model.py --------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Upsample(scale_factor=2, mode='nearest'),\n        conv3x3(in_planes, out_planes),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(True))\n    return block\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num),\n            nn.ReLU(True),\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num))\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        out = self.relu(out)\n        return out\nclass CA_NET(nn.Module):\n    # some code is modified from vae examples\n    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n    def __init__(self):\n        super(CA_NET, self).__init__()\n        self.t_dim = args.DIMENSION\n        self.c_dim = args.CONDITION_DIM\n        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n        self.relu = nn.ReLU()\n\n    def encode(self, text_embedding):\n        x = self.relu(self.fc(text_embedding))\n        mu = x[:, :self.c_dim]\n        logvar = x[:, self.c_dim:]\n        return mu, logvar\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        if args.CUDA:\n            eps = torch.cuda.FloatTensor(std.size()).normal_()\n        else:\n            eps = torch.FloatTensor(std.size()).normal_()\n        eps = Variable(eps)\n        return eps.mul(std).add_(mu)\n\n    def forward(self, text_embedding):\n        mu, logvar = self.encode(text_embedding)\n        c_code = self.reparametrize(mu, logvar)\n        return c_code, mu, logvar\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf, nef, bcondition=True):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n        self.ef_dim = nef\n        self.bcondition = bcondition\n        if bcondition:\n            self.outlogits = nn.Sequential(\n                conv3x3(ndf * 8 + nef, ndf * 8),\n                nn.BatchNorm2d(ndf * 8),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n                nn.Sigmoid())\n        else:\n            self.outlogits = nn.Sequential(\n                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n                nn.Sigmoid())\n\n    def forward(self, h_code, c_code=None):\n        # conditioning output\n        if self.bcondition and c_code is not None:\n            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n            c_code = c_code.repeat(1, 1, 4, 4)\n            # state size (ngf+egf) x 4 x 4\n            h_c_code = torch.cat((h_code, c_code), 1)\n        else:\n            h_c_code = h_code\n\n        output = self.outlogits(h_c_code)\n        return output.view(-1)\n\n\n# ############# Networks for stageI GAN #############\nclass STAGE1_G(nn.Module):\n    def __init__(self):\n        super(STAGE1_G, self).__init__()\n        self.gf_dim = args.GF_DIM * 8\n        self.ef_dim = args.CONDITION_DIM\n        self.z_dim = args.Z_DIM\n        self.define_module()\n\n    def define_module(self):\n        ninput = self.z_dim + self.ef_dim\n        ngf = self.gf_dim\n        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n\n        # -> ngf x 4 x 4\n        self.fc = nn.Sequential(\n            nn.Linear(ninput, ngf * 4 * 4, bias=False),\n            nn.BatchNorm1d(ngf * 4 * 4),\n            nn.ReLU(True))\n\n        # ngf x 4 x 4 -> ngf/2 x 8 x 8\n        self.upsample1 = upBlock(ngf, ngf // 2)\n        # -> ngf/4 x 16 x 16\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        # -> ngf/8 x 32 x 32\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        # -> ngf/16 x 64 x 64\n        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n        # -> 3 x 64 x 64\n        self.img = nn.Sequential(\n            conv3x3(ngf // 16, 3),\n            nn.Tanh())\n\n    def forward(self, text_embedding, noise):\n        c_code, mu, logvar = self.ca_net(text_embedding)\n        z_c_code = torch.cat((noise, c_code), 1)\n        h_code = self.fc(z_c_code)\n\n        h_code = h_code.view(-1, self.gf_dim, 4, 4)\n        h_code = self.upsample1(h_code)\n        h_code = self.upsample2(h_code)\n        h_code = self.upsample3(h_code)\n        h_code = self.upsample4(h_code)\n        # state size 3 x 64 x 64\n        fake_img = self.img(h_code)\n        return None, fake_img, mu, logvar\n\n\nclass STAGE1_D(nn.Module):\n    def __init__(self):\n        super(STAGE1_D, self).__init__()\n        self.df_dim = args.DF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.define_module()\n\n    def define_module(self):\n        ndf, nef = self.df_dim, self.ef_dim\n        self.encode_img = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size (ndf*2) x 16 x 16\n            nn.Conv2d(ndf*2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size (ndf*4) x 8 x 8\n            nn.Conv2d(ndf*4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            # state size (ndf * 8) x 4 x 4)\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        self.get_cond_logits = D_GET_LOGITS(ndf, nef)\n        self.get_uncond_logits = None\n\n    def forward(self, image):\n        img_embedding = self.encode_img(image)\n\n        return img_embedding\n\n\n# ############# Networks for stageII GAN #############\nclass STAGE2_G(nn.Module):\n    def __init__(self, STAGE1_G):\n        super(STAGE2_G, self).__init__()\n        self.gf_dim = args.GF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.z_dim = args.Z_DIM\n        self.STAGE1_G = STAGE1_G\n        # fix parameters of stageI GAN\n        for param in self.STAGE1_G.parameters():\n            param.requires_grad = False\n        self.define_module()\n\n    def _make_layer(self, block, channel_num):\n        layers = []\n        for i in range(args.R_NUM):\n            layers.append(block(channel_num))\n        return nn.Sequential(*layers)\n\n    def define_module(self):\n        ngf = self.gf_dim\n        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n        # --> 4ngf x 16 x 16\n        self.encoder = nn.Sequential(\n            conv3x3(3, ngf),\n            nn.ReLU(True),\n            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True))\n        self.hr_joint = nn.Sequential(\n            conv3x3(self.ef_dim + ngf * 4, ngf * 4),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True))\n        self.residual = self._make_layer(ResBlock, ngf * 4)\n        # --> 2ngf x 32 x 32\n        self.upsample1 = upBlock(ngf * 4, ngf * 2)\n        # --> ngf x 64 x 64\n        self.upsample2 = upBlock(ngf * 2, ngf)\n        # --> ngf // 2 x 128 x 128\n        self.upsample3 = upBlock(ngf, ngf // 2)\n        # --> ngf // 4 x 256 x 256\n        self.upsample4 = upBlock(ngf // 2, ngf // 4)\n        # --> 3 x 256 x 256\n        self.img = nn.Sequential(\n            conv3x3(ngf // 4, 3),\n            nn.Tanh())\n\n    def forward(self, text_embedding, noise):\n        _, stage1_img, _, _ = self.STAGE1_G(text_embedding, noise)\n        stage1_img = stage1_img.detach()\n        encoded_img = self.encoder(stage1_img)\n\n        c_code, mu, logvar = self.ca_net(text_embedding)\n        c_code = c_code.view(-1, self.ef_dim, 1, 1)\n        c_code = c_code.repeat(1, 1, 16, 16)\n        i_c_code = torch.cat([encoded_img, c_code], 1)\n        h_code = self.hr_joint(i_c_code)\n        h_code = self.residual(h_code)\n\n        h_code = self.upsample1(h_code)\n        h_code = self.upsample2(h_code)\n        h_code = self.upsample3(h_code)\n        h_code = self.upsample4(h_code)\n\n        fake_img = self.img(h_code)\n        return stage1_img, fake_img, mu, logvar\n\n\nclass STAGE2_D(nn.Module):\n    def __init__(self):\n        super(STAGE2_D, self).__init__()\n        self.df_dim = args.DF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.define_module()\n\n    def define_module(self):\n        ndf, nef = self.df_dim, self.ef_dim\n        self.encode_img = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 128 * 128 * ndf\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),  # 64 * 64 * ndf * 2\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),  # 32 * 32 * ndf * 4\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),  # 16 * 16 * ndf * 8\n            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),  # 8 * 8 * ndf * 16\n            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 32),\n            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 32\n            conv3x3(ndf * 32, ndf * 16),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),   # 4 * 4 * ndf * 16\n            conv3x3(ndf * 16, ndf * 8),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True)   # 4 * 4 * ndf * 8\n        )\n\n        self.get_cond_logits = D_GET_LOGITS(ndf, nef, bcondition=True)\n        self.get_uncond_logits = D_GET_LOGITS(ndf, nef, bcondition=False)\n\n    def forward(self, image):\n        img_embedding = self.encode_img(image)\n\n        return img_embedding\n    \n# ------------------------------------------------  trainer.py --------------------------------------------------\n\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport os\nimport time\n\nimport numpy as np\n#import torchfile\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import save_image\n\nclass GANTrainer(object):\n    def __init__(self, output_dir):\n        if args.FLAG:\n            self.model_dir = os.path.join(output_dir, 'Model')\n            self.image_dir = os.path.join(output_dir, 'Image')\n            self.log_dir = os.path.join(output_dir, 'Log')\n            mkdir_p(self.model_dir)\n            mkdir_p(self.image_dir)\n            mkdir_p(self.log_dir)\n            #self.summary_writer = FileWriter(self.log_dir)\n\n        self.max_epoch = args.MAX_EPOCH\n        self.snapshot_interval = args.SNAPSHOT_INTERVAL\n\n        s_gpus = args.GPU_ID.split(',')\n        self.gpus = [int(ix) for ix in s_gpus]\n        self.num_gpus = len(self.gpus)\n        self.batch_size = args.BATCH_SIZE * self.num_gpus\n        self.output_dir = output_dir\n        #print(self.gpus[0])\n        #torch.cuda.set_device(self.gpus[0])\n        cudnn.benchmark = True\n        \n        # ############# For training stageI GAN #############\n    def load_network_stageI(self):\n        #from model import STAGE1_G, STAGE1_D\n        netG = STAGE1_G()\n        netG.apply(weights_init)\n        print(netG)\n        netD = STAGE1_D()\n        netD.apply(weights_init)\n        print(netD)\n        print('***********************************************************')\n\n        if args.NET_G != '':\n            #state_dict = torch.load(args.NET_G, map_location=lambda storage, loc: storage)\n            #netG.load_state_dict(state_dict)\n            print('generator 1')\n            print('Load from: ', args.NET_G)\n        if args.NET_D != '':\n            #state_dict = torch.load(args.NET_D,map_location=lambda storage, loc: storage)\n            #netD.load_state_dict(state_dict)\n            print('discriminator 1')\n            print('Load from: ', args.NET_D)\n        if args.CUDA:\n            netG.cuda()\n            netD.cuda()\n        return netG, netD\n        \n    def load_network_stageII(self):\n        #from model import STAGE1_G, STAGE2_G, STAGE2_D\n\n        Stage1_G = STAGE1_G()\n        netG = STAGE2_G(Stage1_G)\n        netG.apply(weights_init)\n        #print(netG)\n        if args.NET_G != '':\n            #state_dict = torch.load(args.NET_G,map_location=lambda storage, loc: storage)\n            #netG.load_state_dict(state_dict)\n            print('Load from: ', args.NET_G)\n        elif args.STAGE1_G != '':\n            state_dict = torch.load(args.STAGE1_G,map_location=lambda storage, loc: storage)\n            netG.STAGE1_G.load_state_dict(state_dict)\n            print('Load from: ', args.STAGE1_G)\n        else:\n            print(\"Please give the Stage1_G path\")\n            return\n\n        netD = STAGE2_D()\n        netD.apply(weights_init)\n        if args.NET_D != '':\n            #state_dict = torch.load(args.NET_D,map_location=lambda storage, loc: storage)\n            #netD.load_state_dict(state_dict)\n            print('Load from: ', args.NET_D)\n        #print(netD)\n\n        if args.CUDA:\n            netG.cuda()\n            netD.cuda()\n        return netG, netD\n    \n    def train(self, data_loader, stage=1):\n        if stage == 1:\n            netG, netD = self.load_network_stageI()\n        else:\n            netG, netD = self.load_network_stageII()\n\n        nz = args.Z_DIM\n        batch_size = self.batch_size\n        noise = Variable(torch.FloatTensor(batch_size, nz))\n        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1),volatile=True)\n        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n        if args.CUDA:\n            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n            real_labels, fake_labels = real_labels.cuda(), fake_labels.cuda()\n\n        generator_lr = args.GENERATOR_LR\n        discriminator_lr = args.DISCRIMINATOR_LR\n        lr_decay_step = args.LR_DECAY_EPOCH\n        optimizerD = optim.Adam(netD.parameters(), lr=args.DISCRIMINATOR_LR, betas=(0.5, 0.999))\n        netG_para = []\n        for p in netG.parameters():\n            if p.requires_grad:\n                netG_para.append(p)\n        optimizerG = optim.Adam(netG_para,lr=args.GENERATOR_LR,betas=(0.5, 0.999))\n        count = 0\n        c = 0\n        for epoch in range(self.max_epoch):\n            if c == 1:\n                break\n            start_t = time.time()\n            if epoch % lr_decay_step == 0 and epoch > 0:\n                generator_lr *= 0.5\n                for param_group in optimizerG.param_groups:\n                    param_group['lr'] = generator_lr\n                discriminator_lr *= 0.5\n                for param_group in optimizerD.param_groups:\n                    param_group['lr'] = discriminator_lr\n            br = 0\n            for i, data in enumerate(data_loader, 0):\n                if br==3:\n                    break\n                ######################################################\n                # (1) Prepare training data\n                ######################################################\n                #print('data: -- ', data)\n                print('---------------------')\n                real_img_cpu, txt_embedding = data\n                #print('embedding:  ', txt_embedding)\n                real_imgs = Variable(real_img_cpu)\n                txt_embedding = Variable(txt_embedding)\n                #print('text_embedding:  ', txt_embedding)\n                if args.CUDA:\n                    real_imgs = real_imgs.cuda()\n                    txt_embedding = txt_embedding.cuda()\n\n                #######################################################\n                # (2) Generate fake images\n                ######################################################\n                noise.data.normal_(0, 1)\n                inputs = (txt_embedding, noise)\n                _, fake_imgs, mu, logvar = \\\n                    nn.parallel.data_parallel(netG, inputs, self.gpus)\n\n                print(fake_imgs[0].detach().shape)\n                plt.imshow(fake_imgs[0].cpu().detach().permute(1, 2, 0))\n                ############################\n                # (3) Update D network\n                ###########################\n                netD.zero_grad()\n                errD, errD_real, errD_wrong, errD_fake = \\\n                    compute_discriminator_loss(netD, real_imgs, fake_imgs,\n                                               real_labels, fake_labels,\n                                               mu, self.gpus)\n                errD.backward()\n                optimizerD.step()\n                ############################\n                # (2) Update G network\n                ###########################\n                netG.zero_grad()\n                errG = compute_generator_loss(netD, fake_imgs,\n                                              real_labels, mu, self.gpus)\n                kl_loss = KL_loss(mu, logvar)\n                errG_total = errG + kl_loss * args.KL\n                errG_total.backward()\n                optimizerG.step()\n                \n                br = br+1\n                \n                count = count + 1\n                if i % 10 == 0:\n                    '''summary_D = summary.scalar('D_loss', errD.data)\n                    summary_D_r = summary.scalar('D_loss_real', errD_real)\n                    summary_D_w = summary.scalar('D_loss_wrong', errD_wrong)\n                    summary_D_f = summary.scalar('D_loss_fake', errD_fake)\n                    summary_G = summary.scalar('G_loss', errG.data)\n                    summary_KL = summary.scalar('KL_loss', kl_loss.data)\n\n                    self.summary_writer.add_summary(summary_D, count)\n                    self.summary_writer.add_summary(summary_D_r, count)\n                    self.summary_writer.add_summary(summary_D_w, count)\n                    self.summary_writer.add_summary(summary_D_f, count)\n                    self.summary_writer.add_summary(summary_G, count)\n                    self.summary_writer.add_summary(summary_KL, count)'''\n\n                    # save the image result for each epoch\n                    inputs = (txt_embedding, fixed_noise)\n                    lr_fake, fake, _, _ = nn.parallel.data_parallel(netG, inputs, self.gpus)\n                    print('real_img_cpu', real_img_cpu.shape)   # real_img_cpu torch.Size([128, 3, 64, 64])\n                    #print('text_embedding:  ', txt_embedding)\n                    print(type(real_img_cpu))\n                    print(real_img_cpu[0].shape)\n                    plt.imshow(real_img_cpu[0].permute(1, 2, 0))\n                    #plt.imshow(real_img_cpu[1].numpy().squeeze(), cmap='RGB')\n                    #plt.imshow(real_img_cpu) \n                    '''save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n                    if lr_fake is not None:\n                        save_img_results(None, lr_fake, epoch, self.image_dir)'''\n                print('br', br)\n            end_t = time.time()\n            c = 1\n            print('''[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_KL: %.4f\n                     Loss_real: %.4f Loss_wrong:%.4f Loss_fake %.4f\n                     Total Time: %.2fsec\n                  '''\n                  % (epoch, self.max_epoch, i, len(data_loader),\n                     errD.data, errG.data, kl_loss.data,\n                     errD_real, errD_wrong, errD_fake, (end_t - start_t)))\n            if epoch % self.snapshot_interval == 0:\n                save_model(netG, netD, epoch, self.model_dir)\n        #\n        #save_model(netG, netD, self.max_epoch, self.model_dir)\n        #\n        #self.summary_writer.close()'''\n        \n    \n\n\n# ------------------------------------------------  main.py --------------------------------------------------\n\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torchvision.transforms as transforms\n\nimport argparse\nimport os\nimport random\nimport sys\nimport pprint\nimport datetime\nimport dateutil\nimport dateutil.tz\n#import torchfile\n    \nclass Struct:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n\nif __name__ == '__main__':\n    params = dict()\n    # parameters\n    parser = argparse.ArgumentParser()\n    params = dict()\n    params['CONFIG_NAME']='stageII'\n    params['DATASET_NAME']='coco'\n    params['EMBEDDING_TYPE']='cnn-rnn'\n    params['GPU_ID']='0'\n    params['CUDA']='TRUE'\n    params['WORKERS']=1\n    params['NET_G']=''\n                    #../input/models/coco_netg_epoch_90/coco_netG_epoch_90.pth\n    params['NET_D']=''\n    params['STAGE1_G']='../input/stackgan-ai/tmp/Model/netG_epoch_0.pth'\n    params['DATA_DIR']='../input/coco-data/coco/coco'\n    params['IMG_DIR'] = '../input/coco-train-val2017/train2014/train2014'\n    params['VIS_COUNT']=64\n    params['Z_DIM']=100\n    params['IMSIZE']=256\n    params['STAGE']=2\n    #TRAIN = edict()\n    params['FLAG']='TRUE'\n    params['BATCH_SIZE']=32\n    params['MAX_EPOCH']=1\n    params['SNAPSHOT_INTERVAL']=1\n    #params['PRETAINED_MODEL']=''\n    #params['PRETRAINED_EPOCH']=600\n    params['LR_DECAY_EPOCH']=20\n    params['DISCRIMINATOR_LR']=0.0002\n    params['GENERATOR_LR']=0.0002\n    params['KL']=2.0\n    #gan\n    params['CONDITION_DIM']=128\n    params['DF_DIM']=96\n    params['GF_DIM']=192\n    params['R_NUM']=2\n    #text\n    params['DIMENSION']=1024\n    #params['manualSeed']=random.randint(1, 10000)\n    args = Struct(**params) #Convert nested Python dict to object\n\n   # s = Struct(**params)\n   # config = s             # parser.parse_args()\n   # print(config)\n    manualSeed = random.randint(1, 10000)\n    random.seed(manualSeed)\n    torch.manual_seed(manualSeed)\n    torch.cuda.manual_seed_all(manualSeed)\n    now = datetime.datetime.now(dateutil.tz.tzlocal())\n    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n    print(timestamp)\n    \n    output_dir = 'tmp/'\n    #output_dir = '../output/%s_%s_%s' % (args.DATASET_NAME, args.CONFIG_NAME, timestamp)\n    print(output_dir)\n    num_gpu = len(args.GPU_ID.split(','))\n    if args.FLAG:\n        image_transform = transforms.Compose([\n            transforms.RandomCrop(args.IMSIZE),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        dataset = TextDataset(args.DATA_DIR, 'train',\n                              imsize=args.IMSIZE,\n                              transform=image_transform)\n        assert dataset\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=args.BATCH_SIZE * num_gpu,\n            drop_last=True, shuffle=True, num_workers=int(args.WORKERS))\n\n        algo = GANTrainer(output_dir)\n        algo.train(dataloader, args.STAGE)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-29T12:03:45.126315Z","iopub.execute_input":"2022-11-29T12:03:45.126622Z","iopub.status.idle":"2022-11-29T12:03:51.542249Z","shell.execute_reply.started":"2022-11-29T12:03:45.126567Z","shell.execute_reply":"2022-11-29T12:03:51.539026Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['coco-train-val2017', 'stackgan-ai', 'coco-data']\n2022_11_29_12_03_45\ntmp/\nchecking...\nLoad filenames from: ../input/coco-data/coco/coco/train/filenames.pickle (499)\n499\nCOCO_train2014_000000078077\nembedding....\nembeddings:  (499, 5, 1024)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 8: 'or_v2\\nq\\x03'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mInvalidHeaderError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m                 \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtarfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFHeaderError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfromtarfile\u001b[0;34m(cls, tarfile)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mfrombuf\u001b[0;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m         \u001b[0mchksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m148\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m156\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchksum\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcalc_chksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeaderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidHeaderError\u001b[0m: invalid header","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mlegacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAX_FORMAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m                 \u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1586\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown compression type %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mtaropen\u001b[0;34m(cls, name, mode, fileobj, **kwargs)\u001b[0m\n\u001b[1;32m   1616\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode must be 'r', 'a', 'w' or 'x'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2306\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2307\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2308\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEmptyHeaderError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mReadError\u001b[0m: invalid header","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-2f8ab4a9e238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGANTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-2f8ab4a9e238>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_loader, stage)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_network_stageI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_network_stageII\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_DIM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-2f8ab4a9e238>\u001b[0m in \u001b[0;36mload_network_stageII\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load from: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNET_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAGE1_G\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAGE1_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAGE1_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load from: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAGE1_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 \u001b[0;31m# .zip is used for torch.jit.save and will throw an un-pickling error here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is a zip archive (did you mean to use torch.jit.load()?)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0;31m# if not a tarfile, reset file offset and proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: ../input/stackgan-ai/tmp/Model/netG_epoch_0.pth is a zip archive (did you mean to use torch.jit.load()?)"],"ename":"RuntimeError","evalue":"../input/stackgan-ai/tmp/Model/netG_epoch_0.pth is a zip archive (did you mean to use torch.jit.load()?)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}