{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom __future__ import print_function\n\nfrom tensorboard import summary\nfrom six.moves import range\nfrom PIL import Image\n\nfrom torch.autograd import Variable\n\nfrom copy import deepcopy\nfrom torch.nn import init\n\n!pip install torchfile\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# ------------------------------------------------  utils.py --------------------------------------------------\n\nimport os\nimport errno\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.utils as vutils\n\n#############################\ndef KL_loss(mu, logvar):\n    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n    KLD = torch.mean(KLD_element).mul_(-0.5)\n    return KLD\n\n\ndef compute_discriminator_loss(netD, real_imgs, fake_imgs,\n                               real_labels, fake_labels,\n                               conditions, gpus):\n    criterion = nn.BCELoss()\n    batch_size = real_imgs.size(0)\n    cond = conditions.detach()\n    fake = fake_imgs.detach()\n    real_features = nn.parallel.data_parallel(netD, (real_imgs), gpus)\n    fake_features = nn.parallel.data_parallel(netD, (fake), gpus)\n    # real pairs\n    inputs = (real_features, cond)\n    real_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_real = criterion(real_logits, real_labels)\n    # wrong pairs\n    inputs = (real_features[:(batch_size-1)], cond[1:])\n    wrong_logits = \\\n        nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_wrong = criterion(wrong_logits, fake_labels[1:])\n    # fake pairs\n    inputs = (fake_features, cond)\n    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_fake = criterion(fake_logits, fake_labels)\n\n    if netD.get_uncond_logits is not None:\n        real_logits = \\\n            nn.parallel.data_parallel(netD.get_uncond_logits,\n                                      (real_features), gpus)\n        fake_logits = \\\n            nn.parallel.data_parallel(netD.get_uncond_logits,\n                                      (fake_features), gpus)\n        uncond_errD_real = criterion(real_logits, real_labels)\n        uncond_errD_fake = criterion(fake_logits, fake_labels)\n        #\n        errD = ((errD_real + uncond_errD_real) / 2. +\n                (errD_fake + errD_wrong + uncond_errD_fake) / 3.)\n        errD_real = (errD_real + uncond_errD_real) / 2.\n        errD_fake = (errD_fake + uncond_errD_fake) / 2.\n    else:\n        errD = errD_real + (errD_fake + errD_wrong) * 0.5\n    return errD, errD_real.data[0], errD_wrong.data[0], errD_fake.data[0]\n\n\ndef compute_generator_loss(netD, fake_imgs, real_labels, conditions, gpus):\n    criterion = nn.BCELoss()\n    cond = conditions.detach()\n    fake_features = nn.parallel.data_parallel(netD, (fake_imgs), gpus)\n    # fake pairs\n    inputs = (fake_features, cond)\n    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_fake = criterion(fake_logits, real_labels)\n    if netD.get_uncond_logits is not None:\n        fake_logits = \\\n            nn.parallel.data_parallel(netD.get_uncond_logits,\n                                      (fake_features), gpus)\n        uncond_errD_fake = criterion(fake_logits, real_labels)\n        errD_fake += uncond_errD_fake\n    return errD_fake\n\n\n#############################\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.fill_(0.0)\n\n\n#############################\ndef save_img_results(data_img, fake, epoch, image_dir):\n    num = args.VIS_COUNT\n    fake = fake[0:num]\n    # data_img is changed to [0,1]\n    if data_img is not None:\n        data_img = data_img[0:num]\n        vutils.save_image(data_img, '%s/real_samples.png' % image_dir, normalize=True)\n        # fake.data is still [-1, 1]\n        vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n    else:\n        vutils.save_image(fake.data, '%s/lr_fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n\n\ndef save_model(netG, netD, epoch, model_dir):\n    torch.save(\n        netG.state_dict(),\n        '%s/netG_epoch_%d.pth' % (model_dir, epoch))\n    torch.save(\n        netD.state_dict(),\n        '%s/netD_epoch_last.pth' % (model_dir))\n    print('Save G/D models')\n\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n# ------------------------------------------------  model.py --------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Upsample(scale_factor=2, mode='nearest'),\n        conv3x3(in_planes, out_planes),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(True))\n    return block\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num),\n            nn.ReLU(True),\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num))\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        out = self.relu(out)\n        return out\nclass CA_NET(nn.Module):\n    # some code is modified from vae examples\n    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n    def __init__(self):\n        super(CA_NET, self).__init__()\n        self.t_dim = args.DIMENSION\n        self.c_dim = args.CONDITION_DIM\n        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n        self.relu = nn.ReLU()\n\n    def encode(self, text_embedding):\n        x = self.relu(self.fc(text_embedding))\n        mu = x[:, :self.c_dim]\n        logvar = x[:, self.c_dim:]\n        return mu, logvar\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        if args.CUDA:\n            eps = torch.cuda.FloatTensor(std.size()).normal_()\n        else:\n            eps = torch.FloatTensor(std.size()).normal_()\n        eps = Variable(eps)\n        return eps.mul(std).add_(mu)\n\n    def forward(self, text_embedding):\n        mu, logvar = self.encode(text_embedding)\n        c_code = self.reparametrize(mu, logvar)\n        return c_code, mu, logvar\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf, nef, bcondition=True):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n        self.ef_dim = nef\n        self.bcondition = bcondition\n        if bcondition:\n            self.outlogits = nn.Sequential(\n                conv3x3(ndf * 8 + nef, ndf * 8),\n                nn.BatchNorm2d(ndf * 8),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n                nn.Sigmoid())\n        else:\n            self.outlogits = nn.Sequential(\n                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n                nn.Sigmoid())\n\n    def forward(self, h_code, c_code=None):\n        # conditioning output\n        if self.bcondition and c_code is not None:\n            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n            c_code = c_code.repeat(1, 1, 4, 4)\n            # state size (ngf+egf) x 4 x 4\n            h_c_code = torch.cat((h_code, c_code), 1)\n        else:\n            h_c_code = h_code\n\n        output = self.outlogits(h_c_code)\n        return output.view(-1)\n\n\n# ############# Networks for stageI GAN #############\nclass STAGE1_G(nn.Module):\n    def __init__(self):\n        super(STAGE1_G, self).__init__()\n        self.gf_dim = args.GF_DIM * 8\n        self.ef_dim = args.CONDITION_DIM\n        self.z_dim = args.Z_DIM\n        self.define_module()\n\n    def define_module(self):\n        ninput = self.z_dim + self.ef_dim\n        ngf = self.gf_dim\n        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n\n        # -> ngf x 4 x 4\n        self.fc = nn.Sequential(\n            nn.Linear(ninput, ngf * 4 * 4, bias=False),\n            nn.BatchNorm1d(ngf * 4 * 4),\n            nn.ReLU(True))\n\n        # ngf x 4 x 4 -> ngf/2 x 8 x 8\n        self.upsample1 = upBlock(ngf, ngf // 2)\n        # -> ngf/4 x 16 x 16\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        # -> ngf/8 x 32 x 32\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        # -> ngf/16 x 64 x 64\n        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n        # -> 3 x 64 x 64\n        self.img = nn.Sequential(\n            conv3x3(ngf // 16, 3),\n            nn.Tanh())\n\n    def forward(self, text_embedding, noise):\n        c_code, mu, logvar = self.ca_net(text_embedding)\n        z_c_code = torch.cat((noise, c_code), 1)\n        h_code = self.fc(z_c_code)\n\n        h_code = h_code.view(-1, self.gf_dim, 4, 4)\n        h_code = self.upsample1(h_code)\n        h_code = self.upsample2(h_code)\n        h_code = self.upsample3(h_code)\n        h_code = self.upsample4(h_code)\n        # state size 3 x 64 x 64\n        fake_img = self.img(h_code)\n        return None, fake_img, mu, logvar\n\n\nclass STAGE1_D(nn.Module):\n    def __init__(self):\n        super(STAGE1_D, self).__init__()\n        self.df_dim = args.DF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.define_module()\n\n    def define_module(self):\n        ndf, nef = self.df_dim, self.ef_dim\n        self.encode_img = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size (ndf*2) x 16 x 16\n            nn.Conv2d(ndf*2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size (ndf*4) x 8 x 8\n            nn.Conv2d(ndf*4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            # state size (ndf * 8) x 4 x 4)\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        self.get_cond_logits = D_GET_LOGITS(ndf, nef)\n        self.get_uncond_logits = None\n\n    def forward(self, image):\n        img_embedding = self.encode_img(image)\n\n        return img_embedding\n\n\n# ############# Networks for stageII GAN #############\nclass STAGE2_G(nn.Module):\n    def __init__(self, STAGE1_G):\n        super(STAGE2_G, self).__init__()\n        self.gf_dim = args.GF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.z_dim = args.Z_DIM\n        self.STAGE1_G = STAGE1_G\n        # fix parameters of stageI GAN\n        for param in self.STAGE1_G.parameters():\n            param.requires_grad = False\n        self.define_module()\n\n    def _make_layer(self, block, channel_num):\n        layers = []\n        for i in range(args.R_NUM):\n            layers.append(block(channel_num))\n        return nn.Sequential(*layers)\n\n    def define_module(self):\n        ngf = self.gf_dim\n        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n        # --> 4ngf x 16 x 16\n        self.encoder = nn.Sequential(\n            conv3x3(3, ngf),\n            nn.ReLU(True),\n            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True))\n        self.hr_joint = nn.Sequential(\n            conv3x3(self.ef_dim + ngf * 4, ngf * 4),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True))\n        self.residual = self._make_layer(ResBlock, ngf * 4)\n        # --> 2ngf x 32 x 32\n        self.upsample1 = upBlock(ngf * 4, ngf * 2)\n        # --> ngf x 64 x 64\n        self.upsample2 = upBlock(ngf * 2, ngf)\n        # --> ngf // 2 x 128 x 128\n        self.upsample3 = upBlock(ngf, ngf // 2)\n        # --> ngf // 4 x 256 x 256\n        self.upsample4 = upBlock(ngf // 2, ngf // 4)\n        # --> 3 x 256 x 256\n        self.img = nn.Sequential(\n            conv3x3(ngf // 4, 3),\n            nn.Tanh())\n\n    def forward(self, text_embedding, noise):\n        _, stage1_img, _, _ = self.STAGE1_G(text_embedding, noise)\n        stage1_img = stage1_img.detach()\n        encoded_img = self.encoder(stage1_img)\n\n        c_code, mu, logvar = self.ca_net(text_embedding)\n        c_code = c_code.view(-1, self.ef_dim, 1, 1)\n        c_code = c_code.repeat(1, 1, 16, 16)\n        i_c_code = torch.cat([encoded_img, c_code], 1)\n        h_code = self.hr_joint(i_c_code)\n        h_code = self.residual(h_code)\n\n        h_code = self.upsample1(h_code)\n        h_code = self.upsample2(h_code)\n        h_code = self.upsample3(h_code)\n        h_code = self.upsample4(h_code)\n\n        fake_img = self.img(h_code)\n        return stage1_img, fake_img, mu, logvar\n\n\nclass STAGE2_D(nn.Module):\n    def __init__(self):\n        super(STAGE2_D, self).__init__()\n        self.df_dim = args.DF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.define_module()\n\n    def define_module(self):\n        ndf, nef = self.df_dim, self.ef_dim\n        self.encode_img = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 128 * 128 * ndf\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),  # 64 * 64 * ndf * 2\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),  # 32 * 32 * ndf * 4\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),  # 16 * 16 * ndf * 8\n            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),  # 8 * 8 * ndf * 16\n            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 32),\n            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 32\n            conv3x3(ndf * 32, ndf * 16),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),   # 4 * 4 * ndf * 16\n            conv3x3(ndf * 16, ndf * 8),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True)   # 4 * 4 * ndf * 8\n        )\n\n        self.get_cond_logits = D_GET_LOGITS(ndf, nef, bcondition=True)\n        self.get_uncond_logits = D_GET_LOGITS(ndf, nef, bcondition=False)\n\n    def forward(self, image):\n        img_embedding = self.encode_img(image)\n\n        return img_embedding\n    \n# ------------------------------------------------  trainer.py --------------------------------------------------\n\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport os\nimport time\n\nimport numpy as np\nimport torchfile\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import save_image\n\nclass GANTrainer(object):\n    def __init__(self, output_dir):\n        '''if cfg.TRAIN.FLAG:\n            self.model_dir = os.path.join(output_dir, 'Model')\n            self.image_dir = os.path.join(output_dir, 'Image')\n            self.log_dir = os.path.join(output_dir, 'Log')\n            mkdir_p(self.model_dir)\n            mkdir_p(self.image_dir)\n            mkdir_p(self.log_dir)\n            self.summary_writer = FileWriter(self.log_dir)'''\n\n        self.max_epoch = args.MAX_EPOCH\n        self.snapshot_interval = args.SNAPSHOT_INTERVAL\n\n        s_gpus = args.GPU_ID.split(',')\n        self.gpus = [int(ix) for ix in s_gpus]\n        self.num_gpus = len(self.gpus)\n        self.batch_size = args.BATCH_SIZE * self.num_gpus\n        self.output_dir = output_dir\n        #print(self.gpus[0])\n        #torch.cuda.set_device(self.gpus[0])\n        cudnn.benchmark = True\n        \n    def load_network_stageII(self):\n        #from model import STAGE1_G, STAGE2_G, STAGE2_D\n\n        Stage1_G = STAGE1_G()\n        netG = STAGE2_G(Stage1_G)\n        netG.apply(weights_init)\n        #print(netG)\n        if args.NET_G != '':\n            state_dict = torch.load(args.NET_G,map_location=lambda storage, loc: storage)\n            netG.load_state_dict(state_dict)\n            print('Load from: ', args.NET_G)\n        elif args.STAGE1_G != '':\n            state_dict = \\\n                torch.load(args.STAGE1_G,\n                           map_location=lambda storage, loc: storage)\n            netG.STAGE1_G.load_state_dict(state_dict)\n            print('Load from: ', args.STAGE1_G)\n        else:\n            print(\"Please give the Stage1_G path\")\n            return\n\n        netD = STAGE2_D()\n        netD.apply(weights_init)\n        if args.NET_D != '':\n            state_dict = \\\n                torch.load(args.NET_D,\n                           map_location=lambda storage, loc: storage)\n            netD.load_state_dict(state_dict)\n            print('Load from: ', args.NET_D)\n        #print(netD)\n\n        if args.CUDA:\n            netG.cuda()\n            netD.cuda()\n        return netG, netD\n        \n    \n    \n    def sample(self, datapath, stage=2):\n        if stage == 1:\n            netG, _ = self.load_network_stageI()\n        else:\n            netG, _ = self.load_network_stageII()\n        netG.eval()\n\n        # Load text embeddings generated from the encoder\n        t_file = torchfile.load(datapath)\n        captions_list = t_file.raw_txt\n        #print(captions_list)\n        embeddings = np.concatenate(t_file.fea_txt, axis=0)\n        num_embeddings = len(captions_list)\n        print('Successfully load sentences from: ', datapath)\n        print('Total number of sentences:', num_embeddings)\n        print('num_embeddings:', num_embeddings, embeddings.shape)\n        # path to save generated samples\n        #save_dir = args.NET_G[:args.NET_G.find('.pth')]\n        #mkdir_p(save_dir)\n        save_dir = '/'\n\n        batch_size = np.minimum(num_embeddings, self.batch_size)\n        nz = args.Z_DIM\n        noise = Variable(torch.FloatTensor(batch_size, nz))\n        if args.CUDA:\n            noise = noise.cuda()\n        count = 0\n        while count < num_embeddings:\n            if count > 2:\n                break\n            iend = count + batch_size\n            if iend > num_embeddings:\n                iend = num_embeddings\n                count = num_embeddings - batch_size\n            embeddings_batch = embeddings[count:iend]\n            captions_batch = captions_list[count:iend]\n            txt_embedding = Variable(torch.FloatTensor(embeddings_batch))\n            if args.CUDA:\n                txt_embedding = txt_embedding.cuda()\n\n            #######################################################\n            # (2) Generate fake images\n            ######################################################\n            noise.data.normal_(0, 1)\n            inputs = (txt_embedding, noise)\n            _, fake_imgs, mu, logvar = \\\n                nn.parallel.data_parallel(netG, inputs, self.gpus)\n            for i in range(batch_size):\n                #ax=None\n                #fig, ax = plt.subplots()\n                #save_name = '../input/models/coco_netG_epoch_90/coco_netG_epoch_90/%d.png' % (count + i)\n                save_name = '%s%d.png' % (save_dir, count + i)\n                save_name = \"tmp\"+save_name\n                print(captions_batch[i])\n                print(save_name)\n                #print(txt_embedding)\n                im = fake_imgs[i].data.cpu().numpy()\n                im = (im + 1.0) * 127.5\n                im = im.astype(np.uint8)\n                print('im', im.shape)\n                im = np.transpose(im, (1, 2, 0))\n                print('im', im.shape)\n                tmpimage = torch.from_numpy(im)\n                print(tmpimage.type())\n                print(type(tmpimage))\n                plt.imshow(tmpimage)\n                #img = torch.from_numpy(im)\n                #plt.imshow(img)\n                #im = np.clip(im, 0, 1)\n                #plt.imshow(im)\n                \n                #im = im.convert('RGB')\n                #im = np.clip(im, 0, 1)\n                #ax.imshow(im)\n                \n                #im = Image.fromarray(im)\n                #im.save(save_name)\n                save_image(tmpimage,save_name)\n            count += batch_size\n            print(count)\n\n\n# ------------------------------------------------  main.py --------------------------------------------------\n\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torchvision.transforms as transforms\n\nimport argparse\nimport os\nimport random\nimport sys\nimport pprint\nimport datetime\nimport dateutil\nimport dateutil.tz\nimport torchfile\n    \nclass Struct:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n\nif __name__ == '__main__':\n    params = dict()\n    # parameters\n    parser = argparse.ArgumentParser()\n    params = dict()\n    params['CONFIG_NAME']='stageII'\n    params['DATASET_NAME']='coco'\n    params['EMBEDDING_TYPE']='cnn-rnn'\n    params['GPU_ID']='0'\n    params['CUDA']='TRUE'\n    params['WORKERS']=4\n    params['NET_G']='../input/models/coco_netG_epoch_90.pth'\n                    #../input/models/coco_netG_epoch_90/coco_netG_epoch_90.pth'\n    params['NET_D']=''\n    #params['STAGE1_G']='../output/coco_stageI/Model/netG_epoch_120.pth'\n    params['DATA_DIR']='.../input/val_captions'\n    params['VIS_COUNT']=64\n    params['Z_DIM']=100\n    params['IMSIZE']=256\n    params['STAGE']=2\n    #TRAIN = edict()\n    params['FLAG']='FALSE'\n    params['BATCH_SIZE']=10\n    params['MAX_EPOCH']=100\n    params['SNAPSHOT_INTERVAL']=5\n    #params['PRETAINED_MODEL']=''\n    #params['PRETRAINED_EPOCH']=600\n    params['LR_DECAY_EPOCH']=20\n    params['DISCRIMINATOR_LR']=0.0002\n    params['GENERATOR_LR']=0.0002\n    params['COEFF.KL']=2.0\n    #gan\n    params['CONDITION_DIM']=128\n    params['DF_DIM']=96\n    params['GF_DIM']=192\n    params['R_NUM']=2\n    #text\n    params['DIMENSION']=1024\n    #params['manualSeed']=random.randint(1, 10000)\n    args = Struct(**params) #Convert nested Python dict to object\n\n   # s = Struct(**params)\n   # config = s             # parser.parse_args()\n   # print(config)\n    manualSeed = random.randint(1, 10000)\n    random.seed(manualSeed)\n    torch.manual_seed(manualSeed)\n    torch.cuda.manual_seed_all(manualSeed)\n    now = datetime.datetime.now(dateutil.tz.tzlocal())\n    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n    print(timestamp)\n    \n    #    output_dir = '../output/%s_%s_%s' % \\(cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n    #output_dir = '../output/args.DATASET_NAME_args.CONFIG_NAME_%imestamp' \n    output_dir = 'tmp/'\n    \n    num_gpu = len(args.GPU_ID.split(','))\n    #num_gpu = 2\n    #print(num_gpu)\n    #datapath= '%s/test/val_captions.t7' % (cfg.DATA_DIR)\n    datapath= '../input/val-captions/test/test/val_captions.t7'\n    algo = GANTrainer(output_dir)\n    algo.sample(datapath, args.STAGE)\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-10T18:40:52.146420Z","iopub.execute_input":"2022-12-10T18:40:52.146727Z","iopub.status.idle":"2022-12-10T18:41:15.748696Z","shell.execute_reply.started":"2022-12-10T18:40:52.146674Z","shell.execute_reply":"2022-12-10T18:41:15.746215Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchfile\n  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\nBuilding wheels for collected packages: torchfile\n  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\nSuccessfully built torchfile\nInstalling collected packages: torchfile\nSuccessfully installed torchfile-0.1.0\n['models']\n2022_12_10_18_41_03\nLoad from:  ../input/models/coco_netG_epoch_90.pth\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f3b583e3a83f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mdatapath\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'../input/val-captions/test/test/val_captions.t7'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGANTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-f3b583e3a83f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, datapath, stage)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;31m# Load text embeddings generated from the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mt_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mcaptions_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;31m#print(captions_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchfile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mT7Reader\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT7Reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/val-captions/test/test/val_captions.t7'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/val-captions/test/test/val_captions.t7'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}