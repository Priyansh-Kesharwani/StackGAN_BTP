{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchfile","metadata":{"execution":{"iopub.status.busy":"2022-12-11T22:22:52.746037Z","iopub.execute_input":"2022-12-11T22:22:52.746363Z","iopub.status.idle":"2022-12-11T22:23:03.489232Z","shell.execute_reply.started":"2022-12-11T22:22:52.746308Z","shell.execute_reply":"2022-12-11T22:23:03.488314Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchfile\n  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\nBuilding wheels for collected packages: torchfile\n  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\nSuccessfully built torchfile\nInstalling collected packages: torchfile\nSuccessfully installed torchfile-0.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom __future__ import print_function\n\nfrom tensorboard import summary\nfrom six.moves import range\nfrom PIL import Image\n\nfrom torch.autograd import Variable\n\nfrom copy import deepcopy\nfrom torch.nn import init\n\n#!pip install torchfile\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# ------------------------------------------------  dataset.py --------------------------------------------------\n\nimport torch.utils.data as data\nfrom PIL import Image\nimport PIL\nimport os\nimport os.path\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\n\nclass TextDataset(data.Dataset):\n    def __init__(self, data_dir, split='train', embedding_type='cnn-rnn',\n                 imsize=64, transform=None, target_transform=None):\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.imsize = imsize\n        self.data = []\n        self.data_dir = data_dir\n        if data_dir.find('birds') != -1:\n            self.bbox = self.load_bbox()\n        else:\n            print('checking...')\n            self.bbox = None\n        split_dir = os.path.join(data_dir, split)\n\n        self.filenames = self.load_filenames(split_dir)\n        self.embeddings = self.load_embedding(split_dir, embedding_type)\n        #self.class_id = self.load_class_id(split_dir, len(self.filenames))\n        # self.captions = self.load_all_captions()\n\n    def get_img(self, img_path, bbox):\n        img = Image.open(img_path).convert('RGB')\n        width, height = img.size\n        if bbox is not None:\n            R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n            center_x = int((2 * bbox[0] + bbox[2]) / 2)\n            center_y = int((2 * bbox[1] + bbox[3]) / 2)\n            y1 = np.maximum(0, center_y - R)\n            y2 = np.minimum(height, center_y + R)\n            x1 = np.maximum(0, center_x - R)\n            x2 = np.minimum(width, center_x + R)\n            img = img.crop([x1, y1, x2, y2])\n        load_size = int(self.imsize * 76 / 64)\n        img = img.resize((load_size, load_size), PIL.Image.BILINEAR)\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def load_bbox(self):\n        data_dir = self.data_dir\n        bbox_path = os.path.join(data_dir, 'CUB_200_2011/bounding_boxes.txt')\n        df_bounding_boxes = pd.read_csv(bbox_path,\n                                        delim_whitespace=True,\n                                        header=None).astype(int)\n        #\n        filepath = os.path.join(data_dir, 'CUB_200_2011/images.txt')\n        df_filenames = \\\n            pd.read_csv(filepath, delim_whitespace=True, header=None)\n        filenames = df_filenames[1].tolist()\n        print('Total filenames: ', len(filenames), filenames[0])\n        #\n        filename_bbox = {img_file[:-4]: [] for img_file in filenames}\n        numImgs = len(filenames)\n        for i in xrange(0, numImgs):\n            # bbox = [x-left, y-top, width, height]\n            bbox = df_bounding_boxes.iloc[i][1:].tolist()\n\n            key = filenames[i][:-4]\n            filename_bbox[key] = bbox\n        #\n        return filename_bbox\n\n    def load_all_captions(self):\n        caption_dict = {}\n        for key in self.filenames:\n            caption_name = '%s/text/%s.txt' % (self.data_dir, key)\n            captions = self.load_captions(caption_name)\n            caption_dict[key] = captions\n        return caption_dict\n\n    def load_captions(self, caption_name):\n        cap_path = caption_name\n        with open(cap_path, \"r\") as f:\n            captions = f.read().decode('utf8').split('\\n')\n        captions = [cap.replace(\"\\ufffd\\ufffd\", \" \")\n                    for cap in captions if len(cap) > 0]\n        return captions\n\n    def load_embedding(self, data_dir, embedding_type):\n        if embedding_type == 'cnn-rnn':\n            print('embedding....')\n            embedding_filename = os.path.join(data_dir, 'char-CNN-RNN-embeddings.pickle')\n            #print(embedding_filename)\n            #embedding_filename = '/char-CNN-RNN-embeddings.pickle'\n        elif embedding_type == 'cnn-gru':\n            embedding_filename = '/char-CNN-GRU-embeddings.pickle'\n        elif embedding_type == 'skip-thought':\n            embedding_filename = '/skip-thought-embeddings.pickle'\n\n        with open(embedding_filename, 'rb') as f:\n            #embeddings = pickle.load(f)\n            embeddings = pickle._Unpickler(f)\n            embeddings.encoding = 'latin1'\n            embeddings = embeddings.load()\n            #print(embeddings)\n            embeddings = np.array(embeddings)\n            # embedding_shape = [embeddings.shape[-1]]\n        '''with open('mnist.pkl', 'rb') as f:\n            u = pickle._Unpickler(f)\n            u.encoding = 'latin1'\n            p = u.load()'''\n            #print('embeddings: ', embeddings.shape)\n        return embeddings\n\n    '''def load_class_id(self, data_dir, total_num):\n        if os.path.isfile(data_dir + '/class_info.pickle'):\n            with open(data_dir + '/class_info.pickle', 'rb') as f:\n                class_id = pickle.load(f)\n        else:\n            class_id = np.arange(total_num)\n        return class_id'''\n    \n\n    def load_filenames(self, data_dir):\n        #../input/coco-data/coco/coco/train/filenames.pickle\n        filepath = os.path.join(data_dir, 'filenames.pickle')\n        with open(filepath, 'rb') as f:\n            filenames = pickle.load(f)\n        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n        print(len(filenames))\n        print(args.IMG_DIR)\n        l=os.listdir('../input/coco-train-val2017/train2014/train2014')\n        im_file=[x.split('.')[0] for x in l]\n        #im_file = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"args.IMG_DIR\") if os.path.isfile(os.path.join('args.IMG_DIR', x))]\n        print(len(im_file))\n        #str = 'COCO_train2014_'\n        #im_file = [str + x for x in im_file]\n        print(im_file[0]) \n        print(filenames[0]) #COCO_train2014_\n        te = list(set(im_file) & set(filenames)) \n        print('te: ', len(te))\n        #list_ = ['.'.join(x.split('.')[:-1]) for x in os.listdir(\"path/to/Data\") if os.path.isfile(os.path.join('path/to/Data', x))]\n        #print(ssd)\n        return filenames\n\n    \n    '''def load_filenames(self, data_dir):\n        #../input/coco-data/coco/coco/train/filenames.pickle\n        filepath = os.path.join(data_dir, 'filenames.pickle')\n        with open(filepath, 'rb') as f:\n            filenames = pickle.load(f)\n        print('Load filenames from: %s (%d)' % (filepath, len(filenames)))\n        return filenames'''\n\n    def __getitem__(self, index):\n        key = self.filenames[index]\n        # cls_id = self.class_id[index]\n        #\n        if self.bbox is not None:\n            bbox = self.bbox[key]\n            data_dir = '%s/CUB_200_2011' % self.data_dir\n        else:\n            bbox = None\n            data_dir = self.data_dir\n\n        # captions = self.captions[key]\n        embeddings = self.embeddings[index, :, :]\n        #print(args.IMG_DIR)\n        #print(key)\n        #y = key[-12:]\n        img_name = '%s/%s.jpg' % (args.IMG_DIR, key)\n        #img_name = '%s/images/%s.jpg' % (data_dir, key)\n        img = self.get_img(img_name, bbox)\n\n        embedding_ix = random.randint(0, embeddings.shape[0]-1)\n        embedding = embeddings[embedding_ix, :]\n        if self.target_transform is not None:\n            embedding = self.target_transform(embedding)\n        return img, embedding\n\n    def __len__(self):\n        return len(self.filenames)\n\n# ------------------------------------------------  utils.py --------------------------------------------------\n\nimport os\nimport errno\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.utils as vutils\n\n#############################\ndef KL_loss(mu, logvar):\n    # -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n    KLD = torch.mean(KLD_element).mul_(-0.5)\n    return KLD\n\n\ndef compute_discriminator_loss(netD, real_imgs, fake_imgs,\n                               real_labels, fake_labels,\n                               conditions, gpus):\n    criterion = nn.BCELoss()\n    batch_size = real_imgs.size(0)\n    cond = conditions.detach()\n    fake = fake_imgs.detach()\n    real_features = nn.parallel.data_parallel(netD, (real_imgs), gpus)\n    fake_features = nn.parallel.data_parallel(netD, (fake), gpus)\n    # real pairs\n    inputs = (real_features, cond)\n    real_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_real = criterion(real_logits, real_labels)\n    # wrong pairs\n    inputs = (real_features[:(batch_size-1)], cond[1:])\n    wrong_logits = \\\n        nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_wrong = criterion(wrong_logits, fake_labels[1:])\n    # fake pairs\n    inputs = (fake_features, cond)\n    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_fake = criterion(fake_logits, fake_labels)\n\n    if netD.get_uncond_logits is not None:\n        real_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (real_features), gpus)\n        fake_logits = nn.parallel.data_parallel(netD.get_uncond_logits, (fake_features), gpus)\n        uncond_errD_real = criterion(real_logits, real_labels)\n        uncond_errD_fake = criterion(fake_logits, fake_labels)\n        #\n        errD = ((errD_real + uncond_errD_real) / 2. + (errD_fake + errD_wrong + uncond_errD_fake) / 3.)\n        errD_real = (errD_real + uncond_errD_real) / 2.\n        errD_fake = (errD_fake + uncond_errD_fake) / 2.\n    else:\n        errD = errD_real + (errD_fake + errD_wrong) * 0.5\n    return errD, errD_real.data, errD_wrong.data, errD_fake.data\n\n\ndef compute_generator_loss(netD, fake_imgs, real_labels, conditions, gpus):\n    criterion = nn.BCELoss()\n    cond = conditions.detach()\n    fake_features = nn.parallel.data_parallel(netD, (fake_imgs), gpus)\n    # fake pairs\n    inputs = (fake_features, cond)\n    fake_logits = nn.parallel.data_parallel(netD.get_cond_logits, inputs, gpus)\n    errD_fake = criterion(fake_logits, real_labels)\n    if netD.get_uncond_logits is not None:\n        fake_logits = \\\n            nn.parallel.data_parallel(netD.get_uncond_logits,\n                                      (fake_features), gpus)\n        uncond_errD_fake = criterion(fake_logits, real_labels)\n        errD_fake += uncond_errD_fake\n    return errD_fake\n\n\n#############################\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        if m.bias is not None:\n            m.bias.data.fill_(0.0)\n\n\n#############################\ndef save_img_results(data_img, fake, epoch, image_dir):\n    num = args.VIS_COUNT\n    fake = fake[0:num]\n    # data_img is changed to [0,1]\n    if data_img is not None:\n        data_img = data_img[0:num]\n        vutils.save_image(data_img, '%s/real_samples.png' % image_dir, normalize=True)\n        # fake.data is still [-1, 1]\n        vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n    else:\n        vutils.save_image(fake.data, '%s/lr_fake_samples_epoch_%03d.png' % (image_dir, epoch), normalize=True)\n\n\ndef save_model(netG, netD, epoch, model_dir):\n    torch.save(\n        netG.state_dict(),\n        '%s/netG_epoch_%d.pth' % (model_dir, epoch))\n    torch.save(\n        netD.state_dict(),\n        '%s/netD_epoch_last.pth' % (model_dir))\n    print('Save G/D models')\n\n\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n# ------------------------------------------------  model.py --------------------------------------------------\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Upsample(scale_factor=2, mode='nearest'),\n        conv3x3(in_planes, out_planes),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(True))\n    return block\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num),\n            nn.ReLU(True),\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num))\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        out = self.relu(out)\n        return out\nclass CA_NET(nn.Module):\n    # some code is modified from vae examples\n    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n    def __init__(self):\n        super(CA_NET, self).__init__()\n        self.t_dim = args.DIMENSION\n        self.c_dim = args.CONDITION_DIM\n        self.fc = nn.Linear(self.t_dim, self.c_dim * 2, bias=True)\n        self.relu = nn.ReLU()\n\n    def encode(self, text_embedding):\n        x = self.relu(self.fc(text_embedding))\n        mu = x[:, :self.c_dim]\n        logvar = x[:, self.c_dim:]\n        return mu, logvar\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        if args.CUDA:\n            eps = torch.cuda.FloatTensor(std.size()).normal_()\n        else:\n            eps = torch.FloatTensor(std.size()).normal_()\n        eps = Variable(eps)\n        return eps.mul(std).add_(mu)\n\n    def forward(self, text_embedding):\n        mu, logvar = self.encode(text_embedding)\n        c_code = self.reparametrize(mu, logvar)\n        return c_code, mu, logvar\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf, nef, bcondition=True):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n        self.ef_dim = nef\n        self.bcondition = bcondition\n        if bcondition:\n            self.outlogits = nn.Sequential(\n                conv3x3(ndf * 8 + nef, ndf * 8),\n                nn.BatchNorm2d(ndf * 8),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n                nn.Sigmoid())\n        else:\n            self.outlogits = nn.Sequential(\n                nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n                nn.Sigmoid())\n\n    def forward(self, h_code, c_code=None):\n        # conditioning output\n        if self.bcondition and c_code is not None:\n            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n            c_code = c_code.repeat(1, 1, 4, 4)\n            # state size (ngf+egf) x 4 x 4\n            h_c_code = torch.cat((h_code, c_code), 1)\n        else:\n            h_c_code = h_code\n\n        output = self.outlogits(h_c_code)\n        return output.view(-1)\n\n\n# ############# Networks for stageI GAN #############\nclass STAGE1_G(nn.Module):\n    def __init__(self):\n        super(STAGE1_G, self).__init__()\n        self.gf_dim = args.GF_DIM * 8\n        self.ef_dim = args.CONDITION_DIM\n        self.z_dim = args.Z_DIM\n        self.define_module()\n\n    def define_module(self):\n        ninput = self.z_dim + self.ef_dim\n        ngf = self.gf_dim\n        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n\n        # -> ngf x 4 x 4\n        self.fc = nn.Sequential(\n            nn.Linear(ninput, ngf * 4 * 4, bias=False),\n            nn.BatchNorm1d(ngf * 4 * 4),\n            nn.ReLU(True))\n\n        # ngf x 4 x 4 -> ngf/2 x 8 x 8\n        self.upsample1 = upBlock(ngf, ngf // 2)\n        # -> ngf/4 x 16 x 16\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        # -> ngf/8 x 32 x 32\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        # -> ngf/16 x 64 x 64\n        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n        # -> 3 x 64 x 64\n        self.img = nn.Sequential(\n            conv3x3(ngf // 16, 3),\n            nn.Tanh())\n\n    def forward(self, text_embedding, noise):\n        c_code, mu, logvar = self.ca_net(text_embedding)\n        z_c_code = torch.cat((noise, c_code), 1)\n        h_code = self.fc(z_c_code)\n\n        h_code = h_code.view(-1, self.gf_dim, 4, 4)\n        h_code = self.upsample1(h_code)\n        h_code = self.upsample2(h_code)\n        h_code = self.upsample3(h_code)\n        h_code = self.upsample4(h_code)\n        # state size 3 x 64 x 64\n        fake_img = self.img(h_code)\n        return None, fake_img, mu, logvar\n\n\nclass STAGE1_D(nn.Module):\n    def __init__(self):\n        super(STAGE1_D, self).__init__()\n        self.df_dim = args.DF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.define_module()\n\n    def define_module(self):\n        ndf, nef = self.df_dim, self.ef_dim\n        self.encode_img = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size (ndf*2) x 16 x 16\n            nn.Conv2d(ndf*2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size (ndf*4) x 8 x 8\n            nn.Conv2d(ndf*4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            # state size (ndf * 8) x 4 x 4)\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n        self.get_cond_logits = D_GET_LOGITS(ndf, nef)\n        self.get_uncond_logits = None\n\n    def forward(self, image):\n        img_embedding = self.encode_img(image)\n\n        return img_embedding\n\n\n# ############# Networks for stageII GAN #############\nclass STAGE2_G(nn.Module):\n    def __init__(self, STAGE1_G):\n        super(STAGE2_G, self).__init__()\n        self.gf_dim = args.GF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.z_dim = args.Z_DIM\n        self.STAGE1_G = STAGE1_G\n        # fix parameters of stageI GAN\n        for param in self.STAGE1_G.parameters():\n            param.requires_grad = False\n        self.define_module()\n\n    def _make_layer(self, block, channel_num):\n        layers = []\n        for i in range(args.R_NUM):\n            layers.append(block(channel_num))\n        return nn.Sequential(*layers)\n\n    def define_module(self):\n        ngf = self.gf_dim\n        # TEXT.DIMENSION -> GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n        # --> 4ngf x 16 x 16\n        self.encoder = nn.Sequential(\n            conv3x3(3, ngf),\n            nn.ReLU(True),\n            nn.Conv2d(ngf, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True))\n        self.hr_joint = nn.Sequential(\n            conv3x3(self.ef_dim + ngf * 4, ngf * 4),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True))\n        self.residual = self._make_layer(ResBlock, ngf * 4)\n        # --> 2ngf x 32 x 32\n        self.upsample1 = upBlock(ngf * 4, ngf * 2)\n        # --> ngf x 64 x 64\n        self.upsample2 = upBlock(ngf * 2, ngf)\n        # --> ngf // 2 x 128 x 128\n        self.upsample3 = upBlock(ngf, ngf // 2)\n        # --> ngf // 4 x 256 x 256\n        self.upsample4 = upBlock(ngf // 2, ngf // 4)\n        # --> 3 x 256 x 256\n        self.img = nn.Sequential(\n            conv3x3(ngf // 4, 3),\n            nn.Tanh())\n\n    def forward(self, text_embedding, noise):\n        _, stage1_img, _, _ = self.STAGE1_G(text_embedding, noise)\n        stage1_img = stage1_img.detach()\n        encoded_img = self.encoder(stage1_img)\n\n        c_code, mu, logvar = self.ca_net(text_embedding)\n        c_code = c_code.view(-1, self.ef_dim, 1, 1)\n        c_code = c_code.repeat(1, 1, 16, 16)\n        i_c_code = torch.cat([encoded_img, c_code], 1)\n        h_code = self.hr_joint(i_c_code)\n        h_code = self.residual(h_code)\n\n        h_code = self.upsample1(h_code)\n        h_code = self.upsample2(h_code)\n        h_code = self.upsample3(h_code)\n        h_code = self.upsample4(h_code)\n\n        fake_img = self.img(h_code)\n        return stage1_img, fake_img, mu, logvar\n\n\nclass STAGE2_D(nn.Module):\n    def __init__(self):\n        super(STAGE2_D, self).__init__()\n        self.df_dim = args.DF_DIM\n        self.ef_dim = args.CONDITION_DIM\n        self.define_module()\n\n    def define_module(self):\n        ndf, nef = self.df_dim, self.ef_dim\n        self.encode_img = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),  # 128 * 128 * ndf\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),  # 64 * 64 * ndf * 2\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),  # 32 * 32 * ndf * 4\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),  # 16 * 16 * ndf * 8\n            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),  # 8 * 8 * ndf * 16\n            nn.Conv2d(ndf * 16, ndf * 32, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 32),\n            nn.LeakyReLU(0.2, inplace=True),  # 4 * 4 * ndf * 32\n            conv3x3(ndf * 32, ndf * 16),\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2, inplace=True),   # 4 * 4 * ndf * 16\n            conv3x3(ndf * 16, ndf * 8),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True)   # 4 * 4 * ndf * 8\n        )\n\n        self.get_cond_logits = D_GET_LOGITS(ndf, nef, bcondition=True)\n        self.get_uncond_logits = D_GET_LOGITS(ndf, nef, bcondition=False)\n\n    def forward(self, image):\n        img_embedding = self.encode_img(image)\n\n        return img_embedding\n    \n# ------------------------------------------------  trainer.py --------------------------------------------------\n\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport os\nimport time\n\nimport numpy as np\nimport torchfile\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import save_image\n\nclass GANTrainer(object):\n    def __init__(self, output_dir):\n        if args.FLAG:\n            self.model_dir = os.path.join(output_dir, 'Model')\n            self.image_dir = os.path.join(output_dir, 'Image')\n            self.log_dir = os.path.join(output_dir, 'Log')\n            mkdir_p(self.model_dir)\n            mkdir_p(self.image_dir)\n            mkdir_p(self.log_dir)\n            #self.summary_writer = FileWriter(self.log_dir)\n\n        self.max_epoch = args.MAX_EPOCH\n        self.snapshot_interval = args.SNAPSHOT_INTERVAL\n\n        s_gpus = args.GPU_ID.split(',')\n        self.gpus = [int(ix) for ix in s_gpus]\n        self.num_gpus = len(self.gpus)\n        self.batch_size = args.BATCH_SIZE * self.num_gpus\n        self.output_dir = output_dir\n        #print(self.gpus[0])\n        #torch.cuda.set_device(self.gpus[0])\n        cudnn.benchmark = True\n        \n        # ############# For training stageI GAN #############\n    def load_network_stageI(self):\n        #from model import STAGE1_G, STAGE1_D\n        netG = STAGE1_G()\n        netG.apply(weights_init)\n        print(netG)\n        netD = STAGE1_D()\n        netD.apply(weights_init)\n        print(netD)\n        print('***********************************************************')\n\n        if args.NET_G != '':\n            #state_dict = torch.load(args.NET_G, map_location=lambda storage, loc: storage)\n            #netG.load_state_dict(state_dict)\n            print('generator 1')\n            print('Load from: ', args.NET_G)\n        if args.NET_D != '':\n            #state_dict = torch.load(args.NET_D,map_location=lambda storage, loc: storage)\n            #netD.load_state_dict(state_dict)\n            print('discriminator 1')\n            print('Load from: ', args.NET_D)\n        if args.CUDA:\n            netG.cuda()\n            netD.cuda()\n        return netG, netD\n        \n    def load_network_stageII(self):\n        #from model import STAGE1_G, STAGE2_G, STAGE2_D\n\n        Stage1_G = STAGE1_G()\n        netG = STAGE2_G(Stage1_G)\n        netG.apply(weights_init)\n        #print(netG)\n        if args.NET_G != '':\n            #state_dict = torch.load(args.NET_G,map_location=lambda storage, loc: storage)\n            #netG.load_state_dict(state_dict)\n            print('Load from: ', args.NET_G)\n        elif args.STAGE1_G != '':\n            #state_dict = torch.load(args.STAGE1_G,map_location=lambda storage, loc: storage)\n            #netG.STAGE1_G.load_state_dict(state_dict)\n            print('Load from: ', args.STAGE1_G)\n        else:\n            print(\"Please give the Stage1_G path\")\n            return\n\n        netD = STAGE2_D()\n        netD.apply(weights_init)\n        if args.NET_D != '':\n            #state_dict = torch.load(args.NET_D,map_location=lambda storage, loc: storage)\n            #netD.load_state_dict(state_dict)\n            print('Load from: ', args.NET_D)\n        #print(netD)\n\n        if args.CUDA:\n            netG.cuda()\n            netD.cuda()\n        return netG, netD\n    \n    def train(self, data_loader, stage=1):\n        if stage == 1:\n            netG, netD = self.load_network_stageI()\n        else:\n            netG, netD = self.load_network_stageII()\n\n        nz = args.Z_DIM\n        batch_size = self.batch_size\n        noise = Variable(torch.FloatTensor(batch_size, nz))\n        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1),volatile=True)\n        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n        if args.CUDA:\n            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n            real_labels, fake_labels = real_labels.cuda(), fake_labels.cuda()\n\n        generator_lr = args.GENERATOR_LR\n        discriminator_lr = args.DISCRIMINATOR_LR\n        lr_decay_step = args.LR_DECAY_EPOCH\n        optimizerD = optim.Adam(netD.parameters(), lr=args.DISCRIMINATOR_LR, betas=(0.5, 0.999))\n        netG_para = []\n        for p in netG.parameters():\n            if p.requires_grad:\n                netG_para.append(p)\n        optimizerG = optim.Adam(netG_para,lr=args.GENERATOR_LR,betas=(0.5, 0.999))\n        count = 0\n        c = 0\n        for epoch in range(self.max_epoch):\n            if c == 1:\n                break\n            start_t = time.time()\n            if epoch % lr_decay_step == 0 and epoch > 0:\n                generator_lr *= 0.5\n                for param_group in optimizerG.param_groups:\n                    param_group['lr'] = generator_lr\n                discriminator_lr *= 0.5\n                for param_group in optimizerD.param_groups:\n                    param_group['lr'] = discriminator_lr\n            br = 0\n            for i, data in enumerate(data_loader, 0):\n                if br==3:\n                    break\n                ######################################################\n                # (1) Prepare training data\n                ######################################################\n                print('data: -- ', data)\n                print('---------------------')\n                real_img_cpu, txt_embedding = data\n                #print('embedding:  ', txt_embedding)\n                real_imgs = Variable(real_img_cpu)\n                txt_embedding = Variable(txt_embedding)\n                #print('text_embedding:  ', txt_embedding)\n                if args.CUDA:\n                    real_imgs = real_imgs.cuda()\n                    txt_embedding = txt_embedding.cuda()\n\n                #######################################################\n                # (2) Generate fake images\n                ######################################################\n                noise.data.normal_(0, 1)\n                inputs = (txt_embedding, noise)\n                _, fake_imgs, mu, logvar = \\\n                    nn.parallel.data_parallel(netG, inputs, self.gpus)\n\n                ############################\n                # (3) Update D network\n                ###########################\n                netD.zero_grad()\n                errD, errD_real, errD_wrong, errD_fake = \\\n                    compute_discriminator_loss(netD, real_imgs, fake_imgs,\n                                               real_labels, fake_labels,\n                                               mu, self.gpus)\n                errD.backward()\n                optimizerD.step()\n                ############################\n                # (2) Update G network\n                ###########################\n                netG.zero_grad()\n                errG = compute_generator_loss(netD, fake_imgs,\n                                              real_labels, mu, self.gpus)\n                kl_loss = KL_loss(mu, logvar)\n                errG_total = errG + kl_loss * args.KL\n                errG_total.backward()\n                optimizerG.step()\n                \n                br = br+1\n                \n                count = count + 1\n                if i % 10 == 0:\n                    '''summary_D = summary.scalar('D_loss', errD.data)\n                    summary_D_r = summary.scalar('D_loss_real', errD_real)\n                    summary_D_w = summary.scalar('D_loss_wrong', errD_wrong)\n                    summary_D_f = summary.scalar('D_loss_fake', errD_fake)\n                    summary_G = summary.scalar('G_loss', errG.data)\n                    summary_KL = summary.scalar('KL_loss', kl_loss.data)\n\n                    self.summary_writer.add_summary(summary_D, count)\n                    self.summary_writer.add_summary(summary_D_r, count)\n                    self.summary_writer.add_summary(summary_D_w, count)\n                    self.summary_writer.add_summary(summary_D_f, count)\n                    self.summary_writer.add_summary(summary_G, count)\n                    self.summary_writer.add_summary(summary_KL, count)'''\n\n                    # save the image result for each epoch\n                    inputs = (txt_embedding, fixed_noise)\n                    lr_fake, fake, _, _ = nn.parallel.data_parallel(netG, inputs, self.gpus)\n                    print('real_img_cpu', real_img_cpu.shape)   # real_img_cpu torch.Size([128, 3, 64, 64])\n                    #print('text_embedding:  ', txt_embedding)\n                    print(type(real_img_cpu))\n                    print(real_img_cpu[0].shape)\n                    plt.imshow(real_img_cpu[0].permute(1, 2, 0))\n                    #plt.imshow(real_img_cpu[1].numpy().squeeze(), cmap='RGB')\n                    #plt.imshow(real_img_cpu) \n                    '''save_img_results(real_img_cpu, fake, epoch, self.image_dir)\n                    if lr_fake is not None:\n                        save_img_results(None, lr_fake, epoch, self.image_dir)'''\n                print('br', br)\n            end_t = time.time()\n            c = 1\n            print('''[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_KL: %.4f\n                     Loss_real: %.4f Loss_wrong:%.4f Loss_fake %.4f\n                     Total Time: %.2fsec\n                  '''\n                  % (epoch, self.max_epoch, i, len(data_loader),\n                     errD.data, errG.data, kl_loss.data,\n                     errD_real, errD_wrong, errD_fake, (end_t - start_t)))\n            if epoch % self.snapshot_interval == 0:\n                save_model(netG, netD, epoch, self.model_dir)\n        #\n        #save_model(netG, netD, self.max_epoch, self.model_dir)\n        #\n        #self.summary_writer.close()'''\n        \n    \n\n\n# ------------------------------------------------  main.py --------------------------------------------------\n\nimport torch.backends.cudnn as cudnn\nimport torch\nimport torchvision.transforms as transforms\n\nimport argparse\nimport os\nimport random\nimport sys\nimport pprint\nimport datetime\nimport dateutil\nimport dateutil.tz\nimport torchfile\n    \nclass Struct:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n\nif __name__ == '__main__':\n    params = dict()\n    # parameters\n    parser = argparse.ArgumentParser()\n    params = dict()\n    params['CONFIG_NAME']='stageI'\n    params['DATASET_NAME']='coco'\n    params['EMBEDDING_TYPE']='cnn-rnn'\n    params['GPU_ID']='0'\n    params['CUDA']='TRUE'\n    params['WORKERS']=4\n    params['NET_G']=''\n                    #../input/models/coco_netg_epoch_90/coco_netG_epoch_90.pth\n    params['NET_D']=''\n    #params['STAGE1_G']='../output/coco_stageI/Model/netG_epoch_120.pth'\n    params['DATA_DIR']='../input/coco-data/coco/coco'\n    params['IMG_DIR'] = '../input/coco-train-val2017/train2014/train2014'\n    params['VIS_COUNT']=64\n    params['Z_DIM']=100\n    params['IMSIZE']=64\n    params['STAGE']=1\n    #TRAIN = edict()\n    params['FLAG']='TRUE'\n    params['BATCH_SIZE']=128\n    params['MAX_EPOCH']=2\n    params['SNAPSHOT_INTERVAL']=1\n    #params['PRETAINED_MODEL']=''\n    #params['PRETRAINED_EPOCH']=600\n    params['LR_DECAY_EPOCH']=20\n    params['DISCRIMINATOR_LR']=0.0002\n    params['GENERATOR_LR']=0.0002\n    params['KL']=2.0\n    #gan\n    params['CONDITION_DIM']=128\n    params['DF_DIM']=96\n    params['GF_DIM']=192\n    #params['R_NUM']=2\n    #text\n    params['DIMENSION']=1024\n    #params['manualSeed']=random.randint(1, 10000)\n    args = Struct(**params) #Convert nested Python dict to object\n\n   # s = Struct(**params)\n   # config = s             # parser.parse_args()\n   # print(config)\n    manualSeed = random.randint(1, 10000)\n    random.seed(manualSeed)\n    torch.manual_seed(manualSeed)\n    torch.cuda.manual_seed_all(manualSeed)\n    now = datetime.datetime.now(dateutil.tz.tzlocal())\n    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n    print(timestamp)\n    \n    output_dir = 'tmp/'\n    #output_dir = '../output/%s_%s_%s' % (args.DATASET_NAME, args.CONFIG_NAME, timestamp)\n    print(output_dir)\n    num_gpu = len(args.GPU_ID.split(','))\n    if args.FLAG:\n        image_transform = transforms.Compose([\n            transforms.RandomCrop(args.IMSIZE),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        dataset = TextDataset(args.DATA_DIR, 'train',\n                              imsize=args.IMSIZE,\n                              transform=image_transform)\n        assert dataset\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=args.BATCH_SIZE * num_gpu,\n            drop_last=True, shuffle=True, num_workers=int(args.WORKERS))\n\n        algo = GANTrainer(output_dir)\n        algo.train(dataloader, args.STAGE)\n        \n#'../input/visda2018-validation/val2017/val2017/000000144280.jpg'\n#'../input/visda2018-validation/val2017/val2017/000000212226.jpg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-11T22:23:06.602900Z","iopub.execute_input":"2022-12-11T22:23:06.603221Z","iopub.status.idle":"2022-12-11T22:23:41.829337Z","shell.execute_reply.started":"2022-12-11T22:23:06.603168Z","shell.execute_reply":"2022-12-11T22:23:41.828221Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['coco-data', 'models', 'coco-train-val2017']\n2022_12_11_22_23_07\ntmp/\nchecking...\nLoad filenames from: ../input/coco-data/coco/coco/train/filenames.pickle (82783)\n82783\n../input/coco-train-val2017/train2014/train2014\n82783\nCOCO_train2014_000000263229\nCOCO_train2014_000000487025\nte:  82783\nembedding....\nSTAGE1_G(\n  (ca_net): CA_NET(\n    (fc): Linear(in_features=1024, out_features=256, bias=True)\n    (relu): ReLU()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=228, out_features=24576, bias=False)\n    (1): BatchNorm1d(24576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace)\n  )\n  (upsample1): Sequential(\n    (0): Upsample(scale_factor=2.0, mode=nearest)\n    (1): Conv2d(1536, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU(inplace)\n  )\n  (upsample2): Sequential(\n    (0): Upsample(scale_factor=2.0, mode=nearest)\n    (1): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU(inplace)\n  )\n  (upsample3): Sequential(\n    (0): Upsample(scale_factor=2.0, mode=nearest)\n    (1): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU(inplace)\n  )\n  (upsample4): Sequential(\n    (0): Upsample(scale_factor=2.0, mode=nearest)\n    (1): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): ReLU(inplace)\n  )\n  (img): Sequential(\n    (0): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (1): Tanh()\n  )\n)\nSTAGE1_D(\n  (encode_img): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (1): LeakyReLU(negative_slope=0.2, inplace)\n    (2): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace)\n    (5): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace)\n    (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace)\n  )\n  (get_cond_logits): D_GET_LOGITS(\n    (outlogits): Sequential(\n      (0): Conv2d(896, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): LeakyReLU(negative_slope=0.2, inplace)\n      (3): Conv2d(768, 1, kernel_size=(4, 4), stride=(4, 4))\n      (4): Sigmoid()\n    )\n  )\n)\n***********************************************************\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:746: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","output_type":"stream"},{"name":"stdout","text":"data: --  [tensor([[[[-0.6392, -0.7647, -0.8353,  ..., -0.5608, -0.6078, -0.3098],\n          [-0.6078, -0.7725, -0.6863,  ..., -0.3412, -0.2235,  0.0118],\n          [-0.6314, -0.7490, -0.5686,  ..., -0.2078, -0.0902,  0.0039],\n          ...,\n          [-0.1294, -0.1137, -0.1294,  ...,  0.0353,  0.0275,  0.0510],\n          [-0.2471, -0.2471, -0.3255,  ...,  0.0353,  0.0431,  0.0510],\n          [-0.1137, -0.1686, -0.2157,  ...,  0.0431,  0.0431,  0.0431]],\n\n         [[-0.8353, -0.8196, -0.7882,  ..., -0.5137, -0.5843, -0.2784],\n          [-0.8353, -0.8510, -0.8275,  ..., -0.2941, -0.1686,  0.0902],\n          [-0.8431, -0.8510, -0.8275,  ..., -0.1216, -0.0039,  0.0980],\n          ...,\n          [-0.1373, -0.1137, -0.1373,  ..., -0.0196, -0.0196, -0.0118],\n          [-0.2471, -0.2471, -0.3176,  ...,  0.0039, -0.0039,  0.0039],\n          [-0.1137, -0.1686, -0.2157,  ...,  0.0039, -0.0039,  0.0039]],\n\n         [[-0.7725, -0.7490, -0.6941,  ..., -0.5216, -0.6235, -0.3098],\n          [-0.7882, -0.7882, -0.7569,  ..., -0.2941, -0.1529,  0.1216],\n          [-0.7882, -0.7882, -0.7647,  ..., -0.0824,  0.0118,  0.1529],\n          ...,\n          [-0.1843, -0.1765, -0.1922,  ..., -0.0980, -0.0902, -0.0824],\n          [-0.2863, -0.2863, -0.3569,  ..., -0.0745, -0.0745, -0.0667],\n          [-0.1686, -0.2235, -0.2627,  ..., -0.0667, -0.0667, -0.0588]]],\n\n\n        [[[-1.0000, -1.0000, -1.0000,  ..., -0.3961, -0.2549,  0.2157],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.4039,  0.2314,  0.0902],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.8118,  0.7490,  0.6706],\n          ...,\n          [-1.0000, -1.0000, -1.0000,  ...,  0.6706,  0.6000,  0.3882],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.8196,  0.8118,  0.7882],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.9294,  0.8196,  0.8353]],\n\n         [[-1.0000, -1.0000, -1.0000,  ..., -0.5765, -0.5451, -0.2000],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.3255,  0.1529, -0.0510],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.7569,  0.7020,  0.5529],\n          ...,\n          [-1.0000, -1.0000, -1.0000,  ...,  0.6392,  0.5765,  0.3569],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.7882,  0.7961,  0.7490],\n          [-1.0000, -1.0000, -1.0000,  ...,  0.9216,  0.8039,  0.8039]],\n\n         [[-1.0000, -1.0000, -1.0000,  ..., -0.8745, -0.9059, -0.8353],\n          [-1.0000, -1.0000, -1.0000,  ..., -0.3412, -0.4118, -0.5843],\n          [-1.0000, -1.0000, -1.0000,  ..., -0.0980, -0.0902, -0.1843],\n          ...,\n          [-1.0000, -1.0000, -1.0000,  ..., -0.4510, -0.4745, -0.6000],\n          [-1.0000, -1.0000, -1.0000,  ..., -0.3490, -0.3255, -0.4039],\n          [-1.0000, -1.0000, -1.0000,  ..., -0.1843, -0.3490, -0.4118]]],\n\n\n        [[[-0.8196, -0.8118, -0.8039,  ..., -0.9059, -0.9137, -0.9137],\n          [-0.8431, -0.8431, -0.8353,  ..., -0.9059, -0.9059, -0.9137],\n          [-0.9137, -0.9059, -0.8980,  ..., -0.9059, -0.9059, -0.9137],\n          ...,\n          [ 0.5059,  0.5137,  0.5137,  ...,  0.4431,  0.4275,  0.3882],\n          [ 0.5216,  0.5059,  0.5137,  ...,  0.4667,  0.4667,  0.4275],\n          [ 0.4902,  0.5137,  0.5294,  ...,  0.4510,  0.4431,  0.4353]],\n\n         [[-0.8196, -0.8118, -0.8039,  ..., -0.9059, -0.9137, -0.9137],\n          [-0.8431, -0.8431, -0.8353,  ..., -0.9059, -0.9059, -0.9137],\n          [-0.9137, -0.9059, -0.8980,  ..., -0.9059, -0.9059, -0.9137],\n          ...,\n          [ 0.3098,  0.3098,  0.2941,  ...,  0.1373,  0.1294,  0.0902],\n          [ 0.2941,  0.2549,  0.2627,  ...,  0.1529,  0.1686,  0.1373],\n          [ 0.2235,  0.2392,  0.2706,  ...,  0.1686,  0.1765,  0.1608]],\n\n         [[-0.8196, -0.8118, -0.8039,  ..., -0.9059, -0.9137, -0.9137],\n          [-0.8431, -0.8431, -0.8353,  ..., -0.9059, -0.9059, -0.9137],\n          [-0.9137, -0.9059, -0.8980,  ..., -0.9059, -0.9059, -0.9137],\n          ...,\n          [ 0.0588,  0.0431,  0.0275,  ..., -0.1922, -0.1922, -0.2235],\n          [ 0.0196, -0.0588, -0.0510,  ..., -0.1922, -0.1608, -0.1843],\n          [-0.1137, -0.0980, -0.0588,  ..., -0.1294, -0.1216, -0.1294]]],\n\n\n        ...,\n\n\n        [[[-0.3569, -0.5373, -0.5608,  ...,  0.2941,  0.2157, -0.3020],\n          [-0.3804, -0.5373, -0.5686,  ...,  0.3176,  0.2392, -0.0745],\n          [-0.3882, -0.5451, -0.5529,  ...,  0.2784,  0.0196, -0.1137],\n          ...,\n          [ 0.0431, -0.0667,  0.0902,  ..., -0.3333, -0.4353, -0.6392],\n          [ 0.0588,  0.0353,  0.0745,  ..., -0.3098, -0.4431, -0.5216],\n          [ 0.0745,  0.0824,  0.0824,  ..., -0.3255, -0.3647, -0.4353]],\n\n         [[-0.4510, -0.5843, -0.6078,  ..., -0.6078, -0.5529, -0.4667],\n          [-0.4667, -0.5843, -0.6078,  ..., -0.6078, -0.5608, -0.2706],\n          [-0.4824, -0.5922, -0.6000,  ..., -0.6157, -0.5529, -0.2549],\n          ...,\n          [-0.0902, -0.1922, -0.0510,  ..., -0.4196, -0.4980, -0.6706],\n          [-0.0824, -0.1059, -0.0745,  ..., -0.4039, -0.5059, -0.5608],\n          [-0.0588, -0.0667, -0.0588,  ..., -0.4118, -0.4431, -0.4980]],\n\n         [[-0.5686, -0.6549, -0.6627,  ..., -0.6471, -0.5922, -0.4980],\n          [-0.5765, -0.6471, -0.6627,  ..., -0.6549, -0.6000, -0.3098],\n          [-0.5922, -0.6549, -0.6627,  ..., -0.6706, -0.6000, -0.2863],\n          ...,\n          [-0.2235, -0.3020, -0.1922,  ..., -0.4902, -0.5529, -0.7020],\n          [-0.2157, -0.2392, -0.2157,  ..., -0.4824, -0.5608, -0.6078],\n          [-0.1922, -0.2000, -0.2000,  ..., -0.4902, -0.5059, -0.5529]]],\n\n\n        [[[-0.4588, -0.4824, -0.4588,  ...,  0.6000,  0.6078,  0.5765],\n          [-0.4667, -0.4588, -0.4275,  ...,  0.5843,  0.5922,  0.5529],\n          [-0.3569, -0.4510, -0.4039,  ...,  0.4510,  0.4353,  0.3490],\n          ...,\n          [ 0.1373,  0.1216,  0.1843,  ...,  0.5137,  0.4824,  0.4275],\n          [ 0.1373,  0.1922,  0.2627,  ...,  0.5059,  0.4667,  0.4431],\n          [ 0.1922,  0.2235,  0.2471,  ...,  0.4745,  0.4510,  0.4353]],\n\n         [[-0.4667, -0.4902, -0.4510,  ...,  0.6392,  0.6392,  0.6078],\n          [-0.4667, -0.4667, -0.4275,  ...,  0.6235,  0.6157,  0.5686],\n          [-0.3490, -0.4510, -0.4039,  ...,  0.4745,  0.4431,  0.3490],\n          ...,\n          [ 0.1529,  0.1451,  0.1922,  ...,  0.5608,  0.5216,  0.4745],\n          [ 0.1451,  0.2078,  0.2706,  ...,  0.5451,  0.5059,  0.4902],\n          [ 0.2078,  0.2392,  0.2549,  ...,  0.5137,  0.4902,  0.4745]],\n\n         [[-0.4588, -0.4980, -0.4353,  ...,  0.6471,  0.6471,  0.6157],\n          [-0.4667, -0.4902, -0.4196,  ...,  0.6235,  0.6000,  0.5373],\n          [-0.3333, -0.4745, -0.3961,  ...,  0.4353,  0.3882,  0.3098],\n          ...,\n          [ 0.2000,  0.1765,  0.2235,  ...,  0.5686,  0.5451,  0.4980],\n          [ 0.1922,  0.2392,  0.3020,  ...,  0.5608,  0.5294,  0.5059],\n          [ 0.2471,  0.2784,  0.2941,  ...,  0.5373,  0.5137,  0.4980]]],\n\n\n        [[[-0.0980, -0.4980, -0.4667,  ...,  0.1843,  0.1451,  0.1137],\n          [-0.0118, -0.3647, -0.2078,  ...,  0.2314,  0.2235,  0.2235],\n          [ 0.1922,  0.1529,  0.2235,  ...,  0.4431,  0.4275,  0.4118],\n          ...,\n          [-0.9216, -0.9216, -0.9216,  ...,  0.3961,  0.3882,  0.3804],\n          [-0.9216, -0.9216, -0.9294,  ...,  0.4039,  0.3961,  0.3882],\n          [-0.9216, -0.9216, -0.9294,  ...,  0.4667,  0.4510,  0.4431]],\n\n         [[-0.0980, -0.4980, -0.4588,  ...,  0.1608,  0.1137,  0.0824],\n          [-0.0039, -0.3725, -0.2000,  ...,  0.2157,  0.2078,  0.2078],\n          [ 0.2235,  0.1843,  0.2627,  ...,  0.4510,  0.4275,  0.4118],\n          ...,\n          [-0.9608, -0.9608, -0.9608,  ...,  0.4039,  0.4039,  0.3961],\n          [-0.9608, -0.9608, -0.9608,  ...,  0.4196,  0.4118,  0.4039],\n          [-0.9608, -0.9608, -0.9608,  ...,  0.4824,  0.4667,  0.4588]],\n\n         [[-0.1373, -0.5373, -0.4902,  ...,  0.0667,  0.0196, -0.0196],\n          [-0.0667, -0.4353, -0.2706,  ...,  0.1059,  0.0980,  0.0980],\n          [ 0.1373,  0.0745,  0.1608,  ...,  0.3490,  0.3255,  0.3020],\n          ...,\n          [-0.9059, -0.9137, -0.9137,  ...,  0.3647,  0.3647,  0.3569],\n          [-0.9137, -0.9137, -0.9137,  ...,  0.3804,  0.3725,  0.3647],\n          [-0.9137, -0.9137, -0.9137,  ...,  0.4588,  0.4353,  0.4275]]]]), tensor([[ 0.0206,  0.0039,  0.2221,  ...,  0.0178,  0.0002,  0.0070],\n        [-0.1191,  0.0597,  0.1398,  ...,  0.0206, -0.0685, -0.0665],\n        [ 0.1175,  0.0403,  0.0537,  ..., -0.0670, -0.0933, -0.0383],\n        ...,\n        [-0.0035, -0.0083,  0.1308,  ..., -0.0301,  0.0579,  0.0106],\n        [ 0.0439, -0.0136,  0.0081,  ..., -0.0600, -0.0212,  0.0362],\n        [ 0.1046, -0.0385,  0.2220,  ..., -0.0580, -0.0104,  0.0046]])]\n---------------------\nreal_img_cpu torch.Size([128, 3, 64, 64])\n<class 'torch.Tensor'>\ntorch.Size([3, 64, 64])\nbr 1\ndata: --  [tensor([[[[-0.9843, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          [-0.9765, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          [-0.9686, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          ...,\n          [ 0.3804,  0.3725,  0.3569,  ..., -0.3725, -0.4275, -0.4431],\n          [ 0.4196,  0.4353,  0.4275,  ..., -0.3255, -0.3961, -0.3804],\n          [ 0.4353,  0.4431,  0.4353,  ..., -0.3412, -0.3882, -0.4353]],\n\n         [[-0.9843, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          [-0.9765, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          [-0.9686, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          ...,\n          [ 0.3255,  0.3020,  0.2863,  ..., -0.3255, -0.3647, -0.3804],\n          [ 0.3725,  0.3725,  0.3569,  ..., -0.2941, -0.3490, -0.3412],\n          [ 0.3804,  0.3804,  0.3647,  ..., -0.3098, -0.3412, -0.3882]],\n\n         [[-0.9843, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          [-0.9765, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          [-0.9686, -0.9765, -0.9765,  ..., -0.9765, -0.9765, -0.9686],\n          ...,\n          [ 0.0431,  0.0196, -0.0275,  ..., -0.4118, -0.4353, -0.4510],\n          [ 0.0902,  0.0745,  0.0353,  ..., -0.3647, -0.3961, -0.4118],\n          [ 0.0667,  0.0667,  0.0275,  ..., -0.3647, -0.3961, -0.4275]]],\n\n\n        [[[ 0.3647,  0.3725,  0.1922,  ...,  0.2784,  0.2549,  0.2157],\n          [ 0.3647,  0.3725,  0.2000,  ...,  0.3098,  0.2784,  0.2392],\n          [ 0.3804,  0.3804,  0.2078,  ...,  0.3333,  0.3020,  0.2627],\n          ...,\n          [-0.1059, -0.0510, -0.0196,  ...,  0.2000,  0.1843,  0.1608],\n          [-0.0275, -0.0588, -0.0902,  ...,  0.1843,  0.1686,  0.1451],\n          [-0.1059, -0.0824, -0.0588,  ...,  0.1765,  0.1451,  0.1216]],\n\n         [[ 0.1373,  0.1451, -0.0588,  ...,  0.0431,  0.0118, -0.0196],\n          [ 0.1373,  0.1373, -0.0588,  ...,  0.0588,  0.0275, -0.0039],\n          [ 0.1373,  0.1373, -0.0510,  ...,  0.0824,  0.0510,  0.0196],\n          ...,\n          [-0.2863, -0.2314, -0.2078,  ..., -0.0353, -0.0588, -0.0745],\n          [-0.2078, -0.2314, -0.2471,  ..., -0.0510, -0.0667, -0.0902],\n          [-0.2471, -0.2314, -0.2235,  ..., -0.0667, -0.0824, -0.1137]],\n\n         [[-0.4667, -0.4745, -0.6471,  ..., -0.4588, -0.4902, -0.5137],\n          [-0.4588, -0.4588, -0.6392,  ..., -0.4353, -0.4588, -0.4824],\n          [-0.4588, -0.4588, -0.6314,  ..., -0.4118, -0.4353, -0.4667],\n          ...,\n          [-0.6157, -0.5765, -0.5608,  ..., -0.4980, -0.5137, -0.5373],\n          [-0.5451, -0.5373, -0.5529,  ..., -0.5137, -0.5373, -0.5529],\n          [-0.5529, -0.5529, -0.5608,  ..., -0.5373, -0.5608, -0.5686]]],\n\n\n        [[[-0.6627, -0.6706, -0.6706,  ..., -0.3882, -0.3804, -0.3882],\n          [-0.6706, -0.6627, -0.6706,  ..., -0.3804, -0.3725, -0.3647],\n          [-0.6549, -0.6627, -0.6627,  ..., -0.3647, -0.3569, -0.3490],\n          ...,\n          [-0.4275, -0.4196, -0.3961,  ...,  0.5451,  0.4431,  0.3333],\n          [-0.4118, -0.4039, -0.3882,  ...,  0.3647,  0.3020,  0.2392],\n          [-0.4039, -0.3882, -0.3804,  ...,  0.2863,  0.2392,  0.1137]],\n\n         [[-0.4980, -0.4902, -0.4824,  ...,  0.0980,  0.1059,  0.1216],\n          [-0.4902, -0.4824, -0.4667,  ...,  0.1137,  0.1294,  0.1294],\n          [-0.4902, -0.4745, -0.4667,  ...,  0.1294,  0.1373,  0.1451],\n          ...,\n          [-0.0275, -0.0039,  0.0118,  ...,  0.2078,  0.0745,  0.1216],\n          [-0.0196,  0.0039,  0.0196,  ...,  0.0431,  0.1294,  0.1137],\n          [-0.0039,  0.0118,  0.0353,  ...,  0.1294,  0.1059,  0.0275]],\n\n         [[-0.0588, -0.0431, -0.0353,  ...,  0.5922,  0.6078,  0.6314],\n          [-0.0588, -0.0510, -0.0431,  ...,  0.6000,  0.6235,  0.6392],\n          [-0.0510, -0.0353, -0.0275,  ...,  0.6314,  0.6392,  0.6549],\n          ...,\n          [ 0.5373,  0.5529,  0.5529,  ..., -0.8667, -0.8980, -0.7412],\n          [ 0.5451,  0.5608,  0.5529,  ..., -0.8588, -0.7333, -0.6941],\n          [ 0.5451,  0.5373,  0.5608,  ..., -0.7412, -0.7020, -0.6863]]],\n\n\n        ...,\n\n\n        [[[-0.2863, -0.2784, -0.3412,  ...,  0.0588,  0.0667,  0.0353],\n          [-0.2549, -0.2078, -0.3176,  ..., -0.0196, -0.0118, -0.0275],\n          [-0.2549, -0.2000, -0.3020,  ..., -0.1059, -0.1137, -0.1059],\n          ...,\n          [-0.1059, -0.0588, -0.1059,  ...,  0.0118,  0.0275,  0.0510],\n          [-0.1373, -0.0667, -0.0980,  ...,  0.0118,  0.0353,  0.0588],\n          [-0.1608, -0.0667, -0.1059,  ...,  0.0353,  0.0667,  0.0745]],\n\n         [[-0.6863, -0.6784, -0.7020,  ..., -0.0902, -0.1059, -0.0980],\n          [-0.6941, -0.6627, -0.6941,  ..., -0.1529, -0.1686, -0.1608],\n          [-0.6706, -0.6392, -0.6863,  ..., -0.2235, -0.2392, -0.2392],\n          ...,\n          [-0.4588, -0.4275, -0.4275,  ..., -0.4118, -0.3804, -0.3647],\n          [-0.4431, -0.4039, -0.4196,  ..., -0.4118, -0.3961, -0.3647],\n          [-0.4431, -0.4039, -0.4196,  ..., -0.3882, -0.3569, -0.3569]],\n\n         [[-0.8902, -0.8980, -0.8667,  ..., -0.1294, -0.1294, -0.1216],\n          [-0.8824, -0.8824, -0.8588,  ..., -0.1765, -0.2000, -0.1922],\n          [-0.8745, -0.8745, -0.8824,  ..., -0.2706, -0.3020, -0.2784],\n          ...,\n          [-0.7176, -0.7020, -0.6784,  ..., -0.7490, -0.7412, -0.7255],\n          [-0.7020, -0.6784, -0.6549,  ..., -0.7569, -0.7412, -0.7412],\n          [-0.6471, -0.6471, -0.6157,  ..., -0.7490, -0.7098, -0.7098]]],\n\n\n        [[[ 0.7961,  0.9686,  0.9922,  ..., -0.9294, -0.9294, -0.9451],\n          [ 0.9216,  0.9843,  0.9922,  ..., -0.8980, -0.8902, -0.9294],\n          [ 0.8588,  0.9765,  0.9843,  ..., -0.9059, -0.8667, -0.9216],\n          ...,\n          [-0.9216, -0.9137, -0.9451,  ..., -0.9059, -0.8824, -0.8588],\n          [-0.9451, -0.9529, -0.9608,  ..., -0.8902, -0.8353, -0.7490],\n          [-0.9529, -0.9608, -0.9529,  ..., -0.8980, -0.8196, -0.7412]],\n\n         [[ 0.3569,  0.5294,  0.5765,  ..., -0.8275, -0.8353, -0.8510],\n          [ 0.1294,  0.3804,  0.4118,  ..., -0.8275, -0.8353, -0.8353],\n          [-0.1216,  0.1765,  0.3333,  ..., -0.8275, -0.8275, -0.8275],\n          ...,\n          [-0.8667, -0.8588, -0.8902,  ..., -0.8902, -0.8902, -0.8745],\n          [-0.8980, -0.8980, -0.9059,  ..., -0.8824, -0.8667, -0.8353],\n          [-0.9059, -0.9059, -0.9059,  ..., -0.8745, -0.8588, -0.8196]],\n\n         [[-0.1922, -0.2000, -0.2941,  ..., -0.7647, -0.7725, -0.7725],\n          [-0.2706, -0.2235, -0.3333,  ..., -0.7569, -0.7725, -0.7804],\n          [-0.5216, -0.5529, -0.7333,  ..., -0.7569, -0.7647, -0.7647],\n          ...,\n          [-0.8353, -0.8275, -0.8510,  ..., -0.8510, -0.8431, -0.8431],\n          [-0.8667, -0.8510, -0.8588,  ..., -0.8431, -0.8431, -0.8353],\n          [-0.8588, -0.8588, -0.8588,  ..., -0.8510, -0.8431, -0.8353]]],\n\n\n        [[[ 0.1373,  0.1294,  0.1294,  ...,  0.0510,  0.0431,  0.0510],\n          [ 0.1451,  0.1373,  0.1373,  ...,  0.0980,  0.0745,  0.0588],\n          [ 0.1529,  0.1608,  0.1843,  ...,  0.1373,  0.1137,  0.0902],\n          ...,\n          [ 0.4824,  0.3725,  0.3647,  ...,  0.3569, -0.1529, -0.0745],\n          [ 0.6941,  0.6863,  0.7882,  ...,  0.2000, -0.1529, -0.0510],\n          [ 0.7490,  0.7176,  0.7569,  ..., -0.0510, -0.1451, -0.0902]],\n\n         [[ 0.6078,  0.6000,  0.6000,  ...,  0.4118,  0.4039,  0.4039],\n          [ 0.6078,  0.6078,  0.6157,  ...,  0.4588,  0.4353,  0.4196],\n          [ 0.6157,  0.6314,  0.6549,  ...,  0.4902,  0.4745,  0.4510],\n          ...,\n          [ 0.2863,  0.2314,  0.2392,  ...,  0.2706, -0.1686, -0.0588],\n          [ 0.4510,  0.4667,  0.5686,  ...,  0.1529, -0.1922, -0.0353],\n          [ 0.5059,  0.4902,  0.5686,  ..., -0.0745, -0.1608, -0.0667]],\n\n         [[ 0.8667,  0.8667,  0.8588,  ...,  0.7490,  0.7412,  0.7490],\n          [ 0.8588,  0.8588,  0.8667,  ...,  0.7569,  0.7412,  0.7333],\n          [ 0.8588,  0.8667,  0.8824,  ...,  0.7647,  0.7490,  0.7255],\n          ...,\n          [ 0.0196, -0.0118,  0.0039,  ...,  0.1686, -0.3333, -0.2784],\n          [ 0.1451,  0.1451,  0.2000,  ...,  0.0275, -0.4039, -0.2471],\n          [ 0.1529,  0.1686,  0.3255,  ..., -0.1922, -0.3490, -0.3098]]]]), tensor([[-0.0292, -0.0665,  0.1268,  ..., -0.0033, -0.0460, -0.0750],\n        [-0.0178,  0.0244, -0.0162,  ..., -0.0418, -0.0316, -0.0700],\n        [ 0.0102, -0.0178,  0.0571,  ...,  0.0072,  0.1036, -0.0102],\n        ...,\n        [ 0.0112,  0.0362, -0.0296,  ...,  0.0014,  0.0635, -0.0303],\n        [ 0.0866,  0.0452, -0.0311,  ..., -0.0308, -0.0583, -0.0689],\n        [-0.1722, -0.0109,  0.1674,  ...,  0.0811,  0.0453,  0.0006]])]\n---------------------\nbr 2\ndata: --  [tensor([[[[-0.7020, -0.6941, -0.6314,  ...,  0.9608,  0.9451,  0.9294],\n          [-0.6784, -0.6941, -0.6392,  ...,  0.9373,  0.9216,  0.8980],\n          [-0.7098, -0.7020, -0.6627,  ...,  0.9137,  0.8980,  0.8745],\n          ...,\n          [-0.4275, -0.5216, -0.5373,  ..., -0.0118, -0.1059, -0.0431],\n          [-0.4196, -0.4353, -0.4431,  ...,  0.0667,  0.0745,  0.0667],\n          [-0.4196, -0.4039, -0.3725,  ...,  0.0118,  0.0431,  0.0745]],\n\n         [[-0.6863, -0.6863, -0.6235,  ...,  0.9216,  0.9137,  0.8980],\n          [-0.6706, -0.6784, -0.6235,  ...,  0.8980,  0.8824,  0.8667],\n          [-0.6941, -0.6941, -0.6392,  ...,  0.8745,  0.8588,  0.8353],\n          ...,\n          [-0.3490, -0.4431, -0.4353,  ..., -0.1765, -0.2471, -0.1922],\n          [-0.3882, -0.3725, -0.3412,  ..., -0.0980, -0.0902, -0.1059],\n          [-0.3569, -0.3176, -0.3255,  ..., -0.1373, -0.1137, -0.0980]],\n\n         [[-0.8902, -0.8745, -0.8431,  ...,  0.9529,  0.9294,  0.9216],\n          [-0.8667, -0.8824, -0.8275,  ...,  0.9294,  0.9137,  0.8902],\n          [-0.9059, -0.8902, -0.8667,  ...,  0.9059,  0.8902,  0.8667],\n          ...,\n          [-0.7882, -0.8196, -0.8510,  ..., -0.3176, -0.4039, -0.3490],\n          [-0.7569, -0.7725, -0.8039,  ..., -0.1843, -0.1843, -0.2078],\n          [-0.7882, -0.7882, -0.7490,  ..., -0.2000, -0.1922, -0.1843]]],\n\n\n        [[[ 0.0824,  0.1059,  0.1294,  ...,  0.3020,  0.3020,  0.2941],\n          [ 0.1059,  0.1216,  0.1294,  ...,  0.3098,  0.3020,  0.3020],\n          [ 0.1216,  0.1294,  0.1294,  ...,  0.3098,  0.3098,  0.3098],\n          ...,\n          [ 0.0824,  0.0824,  0.1137,  ...,  0.1137,  0.1294,  0.1294],\n          [ 0.0745,  0.0980,  0.1059,  ...,  0.2157,  0.1529,  0.1373],\n          [ 0.0824,  0.1059,  0.0824,  ...,  0.4980,  0.5216,  0.4510]],\n\n         [[ 0.1765,  0.1843,  0.1922,  ...,  0.3412,  0.3412,  0.3333],\n          [ 0.1843,  0.1922,  0.2000,  ...,  0.3490,  0.3490,  0.3412],\n          [ 0.1922,  0.1922,  0.2000,  ...,  0.3490,  0.3490,  0.3490],\n          ...,\n          [ 0.1922,  0.1843,  0.2078,  ...,  0.2078,  0.2235,  0.2235],\n          [ 0.1843,  0.2000,  0.2000,  ...,  0.2863,  0.2314,  0.2235],\n          [ 0.1922,  0.2078,  0.1922,  ...,  0.5451,  0.5686,  0.4902]],\n\n         [[ 0.3098,  0.3176,  0.3333,  ...,  0.0902,  0.0902,  0.0824],\n          [ 0.3176,  0.3255,  0.3412,  ...,  0.1059,  0.0902,  0.0902],\n          [ 0.3255,  0.3333,  0.3333,  ...,  0.0980,  0.0980,  0.0980],\n          ...,\n          [ 0.3333,  0.3333,  0.3412,  ...,  0.3490,  0.3569,  0.3569],\n          [ 0.3255,  0.3255,  0.3333,  ...,  0.4196,  0.3647,  0.3569],\n          [ 0.3333,  0.3490,  0.3333,  ...,  0.6157,  0.6392,  0.5843]]],\n\n\n        [[[ 0.0980,  0.1059,  0.1137,  ..., -0.0431, -0.0510, -0.0588],\n          [ 0.0824,  0.0902,  0.0902,  ..., -0.0431, -0.0510, -0.0588],\n          [-0.0039, -0.0118, -0.0118,  ..., -0.0431, -0.0510, -0.0588],\n          ...,\n          [ 0.8510,  0.8431,  0.8588,  ..., -0.2314, -0.2627, -0.2706],\n          [ 0.9765,  0.9843,  0.9686,  ...,  0.1922,  0.1294,  0.0902],\n          [ 0.9922,  0.9922,  0.9922,  ...,  0.5451,  0.5059,  0.4510]],\n\n         [[ 0.0980,  0.1059,  0.1137,  ..., -0.1451, -0.1451, -0.1608],\n          [ 0.0902,  0.0902,  0.0902,  ..., -0.1373, -0.1373, -0.1451],\n          [-0.0431, -0.0510, -0.0588,  ..., -0.1294, -0.1373, -0.1451],\n          ...,\n          [ 0.9137,  0.9216,  0.9373,  ..., -0.2471, -0.2941, -0.3098],\n          [ 0.9843,  0.9843,  0.9843,  ...,  0.2471,  0.1765,  0.1216],\n          [ 0.9922,  1.0000,  0.9922,  ...,  0.6157,  0.5843,  0.5294]],\n\n         [[ 0.0980,  0.0902,  0.0980,  ..., -0.2549, -0.2627, -0.2706],\n          [ 0.0588,  0.0667,  0.0667,  ..., -0.2471, -0.2549, -0.2627],\n          [-0.1216, -0.1451, -0.1529,  ..., -0.2392, -0.2549, -0.2627],\n          ...,\n          [ 0.9686,  0.9686,  0.9686,  ..., -0.2863, -0.3412, -0.3569],\n          [ 0.9922,  0.9922,  0.9922,  ...,  0.3176,  0.2392,  0.1765],\n          [ 0.9922,  1.0000,  0.9922,  ...,  0.7412,  0.7020,  0.6392]]],\n\n\n        ...,\n\n\n        [[[-0.6549, -0.6549, -0.6000,  ...,  0.1529,  0.1765,  0.2000],\n          [-0.6235, -0.6157, -0.5843,  ...,  0.2157,  0.2157,  0.2784],\n          [-0.5529, -0.5137, -0.4275,  ...,  0.3333,  0.3255,  0.2863],\n          ...,\n          [-0.8588, -0.4431, -0.3725,  ..., -0.5294, -0.4745, -0.3804],\n          [-0.7725, -0.4118, -0.3098,  ..., -0.5216, -0.4902, -0.4353],\n          [-0.7020, -0.4824, -0.4353,  ..., -0.5137, -0.4902, -0.4431]],\n\n         [[-0.4196, -0.4196, -0.3804,  ...,  0.3490,  0.3647,  0.3882],\n          [-0.3961, -0.4118, -0.3725,  ...,  0.3961,  0.4118,  0.4510],\n          [-0.3412, -0.2941, -0.2157,  ...,  0.4824,  0.4824,  0.4588],\n          ...,\n          [-0.7725, -0.2000, -0.0588,  ..., -0.3333, -0.2863, -0.1608],\n          [-0.6235, -0.1373, -0.0980,  ..., -0.3098, -0.2941, -0.2235],\n          [-0.5137, -0.3098, -0.2314,  ..., -0.3255, -0.2784, -0.2392]],\n\n         [[ 0.0353,  0.0039, -0.0039,  ...,  0.3176,  0.3333,  0.3725],\n          [ 0.0824,  0.0431,  0.0431,  ...,  0.3804,  0.3804,  0.4431],\n          [ 0.1451,  0.1529,  0.1922,  ...,  0.4824,  0.4824,  0.4510],\n          ...,\n          [-0.6314,  0.0353,  0.2157,  ..., -0.1765, -0.1294, -0.0275],\n          [-0.4196,  0.1216,  0.1373,  ..., -0.1608, -0.1373, -0.0824],\n          [-0.2157, -0.0353,  0.0118,  ..., -0.1686, -0.1294, -0.0902]]],\n\n\n        [[[-0.3020, -0.2941, -0.2941,  ..., -0.9922, -1.0000, -1.0000],\n          [-0.2863, -0.2784, -0.2941,  ..., -1.0000, -1.0000, -0.9922],\n          [-0.2784, -0.2863, -0.3020,  ..., -0.9922, -0.9922, -0.9922],\n          ...,\n          [-0.3569, -0.3647, -0.3647,  ..., -0.8980, -0.9059, -0.9059],\n          [-0.3725, -0.3725, -0.3647,  ..., -0.8980, -0.8980, -0.9059],\n          [-0.3647, -0.3647, -0.3647,  ..., -0.8902, -0.8980, -0.9059]],\n\n         [[-0.5137, -0.4980, -0.4902,  ..., -0.9922, -1.0000, -1.0000],\n          [-0.4824, -0.4745, -0.4667,  ..., -1.0000, -1.0000, -0.9922],\n          [-0.4588, -0.4588, -0.4667,  ..., -0.9922, -0.9922, -0.9922],\n          ...,\n          [-0.4196, -0.4275, -0.4118,  ..., -0.9373, -0.9451, -0.9373],\n          [-0.4275, -0.4275, -0.4118,  ..., -0.9373, -0.9373, -0.9451],\n          [-0.4353, -0.4275, -0.4118,  ..., -0.9294, -0.9373, -0.9451]],\n\n         [[-0.5686, -0.5686, -0.5686,  ..., -0.9922, -1.0000, -1.0000],\n          [-0.5373, -0.5294, -0.5294,  ..., -1.0000, -1.0000, -0.9922],\n          [-0.5216, -0.5216, -0.5294,  ..., -0.9922, -0.9922, -0.9922],\n          ...,\n          [-0.4039, -0.4118, -0.3961,  ..., -0.9059, -0.9216, -0.9294],\n          [-0.4118, -0.4039, -0.3882,  ..., -0.9059, -0.9059, -0.9137],\n          [-0.4118, -0.4039, -0.3882,  ..., -0.8980, -0.9059, -0.9137]]],\n\n\n        [[[ 0.1294,  0.1294,  0.1294,  ...,  0.5608,  0.3804,  0.2706],\n          [ 0.1529,  0.1765,  0.1686,  ...,  0.5373,  0.4039,  0.3412],\n          [ 0.1765,  0.1765,  0.1686,  ...,  0.5137,  0.4275,  0.3882],\n          ...,\n          [-0.1608, -0.2078, -0.3020,  ..., -0.8431, -0.7961, -0.7804],\n          [-0.1608, -0.2706, -0.3490,  ..., -0.8431, -0.8353, -0.7569],\n          [-0.2706, -0.3412, -0.3882,  ..., -0.8039, -0.7961, -0.7569]],\n\n         [[ 0.3961,  0.4039,  0.3961,  ...,  0.5765,  0.4980,  0.4745],\n          [ 0.4039,  0.4039,  0.4118,  ...,  0.5451,  0.4824,  0.4588],\n          [ 0.3882,  0.4118,  0.4039,  ...,  0.5294,  0.4980,  0.4824],\n          ...,\n          [-0.3725, -0.4118, -0.4824,  ..., -0.8667, -0.8275, -0.8196],\n          [-0.3647, -0.4667, -0.5216,  ..., -0.8745, -0.8588, -0.7961],\n          [-0.4039, -0.5059, -0.5451,  ..., -0.8431, -0.8275, -0.8039]],\n\n         [[ 0.6392,  0.6392,  0.6314,  ...,  0.5922,  0.6235,  0.6471],\n          [ 0.6314,  0.6157,  0.6314,  ...,  0.5608,  0.5765,  0.5843],\n          [ 0.5765,  0.6157,  0.6157,  ...,  0.5529,  0.5843,  0.5922],\n          ...,\n          [-0.5843, -0.6235, -0.6549,  ..., -0.8902, -0.8510, -0.8431],\n          [-0.5843, -0.6627, -0.6863,  ..., -0.8902, -0.8824, -0.8275],\n          [-0.7020, -0.7098, -0.7098,  ..., -0.8588, -0.8510, -0.8275]]]]), tensor([[ 0.0317, -0.0589,  0.1467,  ..., -0.0465, -0.0200,  0.0199],\n        [-0.0548,  0.0299, -0.1337,  ...,  0.0469,  0.0494, -0.0479],\n        [ 0.0347, -0.0031,  0.2692,  ...,  0.0757, -0.0419, -0.0497],\n        ...,\n        [-0.0215,  0.0161,  0.2587,  ...,  0.1157,  0.0133, -0.0436],\n        [ 0.2552,  0.0914,  0.1879,  ...,  0.0253,  0.0206,  0.0207],\n        [-0.2443, -0.0079,  0.1836,  ...,  0.1268,  0.0270, -0.0014]])]\n---------------------\nbr 3\n[0/2][3/646] Loss_D: 6.1057 Loss_G: 0.9971 Loss_KL: 0.0031\n                     Loss_real: 6.0143 Loss_wrong:0.0639 Loss_fake 0.1189\n                     Total Time: 12.08sec\n                  \nSave G/D models\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}